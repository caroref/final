{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S3 Housing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caroref/final/blob/master/S3_Housing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ascUYWYXFlTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "bf7b8e4c-8f70-4e2c-d8ca-26e5f8db92a3"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TensorFlow version is \", tf.__version__)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0604 16:39:40.781626 140700095268736 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version is  1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs-4SeS3Fpc5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "ad16bff6-31d9-4aa7-9f42-e22b25f8a291"
      },
      "source": [
        "# Allow Colab to access drive\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "#linking Colab to use drive as if local\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "E: The repository 'https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Release' is no longer signed.\n",
            "E: The repository 'https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Release' is no longer signed.\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDNLdWVxP2jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main directories\n",
        "base_dir = 'drive/Colab/EV/Base/'\n",
        "like = os.path.join(base_dir, 'Like')\n",
        "dislike = os.path.join(base_dir, 'Dislike')\n",
        "\n",
        "trn_dir = 'drive/Colab/EV/S3/Train/'\n",
        "tst_dir = 'drive/Colab/EV/S3/Test/'\n",
        "\n",
        "#subdirectories\n",
        "trn_like = os.path.join(trn_dir, 'Like')\n",
        "tst_like = os.path.join(tst_dir, 'Like')\n",
        "\n",
        "trn_disl = os.path.join(trn_dir, 'Dislike')\n",
        "tst_disl = os.path.join(tst_dir, 'Dislike')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo3GVjj1FsEn",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5BtzOSmbZB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All images will be resized to image_size\n",
        "\n",
        "image_size = 640 \n",
        "batch_size = 100\n",
        "\n",
        "# create Image Data Generator for Image Data Augmentation\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "                rescale=1./255, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTvB7EieGDAN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6d366fad-d51f-4760-dd6a-d73a77d1acd4"
      },
      "source": [
        "# Create the train and validation generators and specify the train dataset directory, \n",
        "# image size, batch size, binary classification, and subset (training or validation).\n",
        "\n",
        "\n",
        "# Flow training images in batches using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,  \n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode ='binary',\n",
        "                subset = 'training') \n",
        "\n",
        "# Flow validation images in batches using validation_datagen generator\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,\n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode='binary',\n",
        "                subset = 'validation')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ur5r8uGWQO",
        "colab_type": "text"
      },
      "source": [
        "# Create the base model from the pre-trained convnets\n",
        "\n",
        "Using the output of last layer (bottleneck layer). Keep other layers frozen (these are specific to their training, less useful here?) Bottleneck layer should be more general.\n",
        "\n",
        "Using Inception v3 model with weights trained on ImageNet.\n",
        "The argument, include_top=False, means that we have a network that isn't including classification layers at the top. This is good for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCLmagoGbc3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "90f3859b-2dc3-49c1-eaf5-2cb62b73ec35"
      },
      "source": [
        "IMG_SHAPE = (image_size, image_size, 3)\n",
        "\n",
        "base_model = keras.applications.inception_v3.InceptionV3(\n",
        "             include_top=False, weights='imagenet', input_shape=IMG_SHAPE)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0604 16:40:46.448300 140700095268736 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wXzV2zlGduw",
        "colab_type": "text"
      },
      "source": [
        "# Feature extraction\n",
        "We will freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "It's important to freeze the convolutional based before we compile and train the model. By freezing (or setting layer.trainable = False), we prevent the weights in these layers from being updated during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCk3KhmGGhaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11821
        },
        "outputId": "de07fc30-b30d-45fc-813c-ddce6c270915"
      },
      "source": [
        "#freeze base\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# Look at the base model architecture\n",
        "base_model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 640, 640, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 319, 319, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 319, 319, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 319, 319, 32) 0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 317, 317, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 317, 317, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 317, 317, 32) 0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 317, 317, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 317, 317, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 317, 317, 64) 0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 158, 158, 64) 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 158, 158, 80) 5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 158, 158, 80) 240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 158, 158, 80) 0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 156, 156, 192 138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 156, 156, 192 576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 156, 156, 192 0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 77, 77, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_8 (Batch (None, 77, 77, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 77, 77, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 77, 77, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_6 (Batch (None, 77, 77, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_9 (Batch (None, 77, 77, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 77, 77, 48)   0           batch_normalization_v1_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 77, 77, 96)   0           batch_normalization_v1_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 77, 77, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 77, 77, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 77, 77, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 77, 77, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_5 (Batch (None, 77, 77, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_7 (Batch (None, 77, 77, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_10 (Batc (None, 77, 77, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_11 (Batc (None, 77, 77, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 77, 77, 32)   0           batch_normalization_v1_11[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 77, 77, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_15 (Batc (None, 77, 77, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_15[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 77, 77, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 77, 77, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_13 (Batc (None, 77, 77, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_16 (Batc (None, 77, 77, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_13[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_16[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 77, 77, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 77, 77, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 77, 77, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 77, 77, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_12 (Batc (None, 77, 77, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_14 (Batc (None, 77, 77, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_17 (Batc (None, 77, 77, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_18 (Batc (None, 77, 77, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_12[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_14[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_17[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_18[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 77, 77, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_22 (Batc (None, 77, 77, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_22[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 77, 77, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 77, 77, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_20 (Batc (None, 77, 77, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_23 (Batc (None, 77, 77, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_20[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_23[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 77, 77, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 77, 77, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 77, 77, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 77, 77, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_19 (Batc (None, 77, 77, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_21 (Batc (None, 77, 77, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_24 (Batc (None, 77, 77, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_25 (Batc (None, 77, 77, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_19[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_21[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_24[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_25[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 77, 77, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 77, 77, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_27 (Batc (None, 77, 77, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_27[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 77, 77, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_28 (Batc (None, 77, 77, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_28[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 38, 38, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 38, 38, 96)   82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_26 (Batc (None, 38, 38, 384)  1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_29 (Batc (None, 38, 38, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 38, 38, 384)  0           batch_normalization_v1_26[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 38, 38, 96)   0           batch_normalization_v1_29[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 38, 38, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 38, 38, 768)  0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_34 (Batc (None, 38, 38, 128)  384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_34[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 38, 38, 128)  114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_35 (Batc (None, 38, 38, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_35[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 38, 38, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_31 (Batc (None, 38, 38, 128)  384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_36 (Batc (None, 38, 38, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_31[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_36[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 38, 38, 128)  114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 38, 38, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_32 (Batc (None, 38, 38, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_37 (Batc (None, 38, 38, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_32[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_37[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 38, 38, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 38, 38, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 38, 38, 192)  172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 38, 38, 192)  172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_30 (Batc (None, 38, 38, 192)  576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_33 (Batc (None, 38, 38, 192)  576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_38 (Batc (None, 38, 38, 192)  576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_39 (Batc (None, 38, 38, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_30[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_33[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_38[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_39[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 38, 38, 768)  0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_44 (Batc (None, 38, 38, 160)  480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_44[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 38, 38, 160)  179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_45 (Batc (None, 38, 38, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_45[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 38, 38, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_41 (Batc (None, 38, 38, 160)  480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_46 (Batc (None, 38, 38, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_41[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_46[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 38, 38, 160)  179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 38, 38, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_42 (Batc (None, 38, 38, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_47 (Batc (None, 38, 38, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_42[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_47[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 38, 38, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 38, 38, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 38, 38, 192)  215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 38, 38, 192)  215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_40 (Batc (None, 38, 38, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_43 (Batc (None, 38, 38, 192)  576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_48 (Batc (None, 38, 38, 192)  576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_49 (Batc (None, 38, 38, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_40[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_43[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_48[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_49[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 38, 38, 768)  0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_54 (Batc (None, 38, 38, 160)  480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_54[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 38, 38, 160)  179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_55 (Batc (None, 38, 38, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_55[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 38, 38, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_51 (Batc (None, 38, 38, 160)  480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_56 (Batc (None, 38, 38, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_51[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_56[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 38, 38, 160)  179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 38, 38, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_52 (Batc (None, 38, 38, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_57 (Batc (None, 38, 38, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_52[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_57[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 38, 38, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 38, 38, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 38, 38, 192)  215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 38, 38, 192)  215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_50 (Batc (None, 38, 38, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_53 (Batc (None, 38, 38, 192)  576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_58 (Batc (None, 38, 38, 192)  576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_59 (Batc (None, 38, 38, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_50[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_53[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_58[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_59[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 38, 38, 768)  0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_64 (Batc (None, 38, 38, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_64[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 38, 38, 192)  258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_65 (Batc (None, 38, 38, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_65[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 38, 38, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_61 (Batc (None, 38, 38, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_66 (Batc (None, 38, 38, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_61[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_66[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 38, 38, 192)  258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 38, 38, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_62 (Batc (None, 38, 38, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_67 (Batc (None, 38, 38, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_62[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_67[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 38, 38, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 38, 38, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 38, 38, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_60 (Batc (None, 38, 38, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_63 (Batc (None, 38, 38, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_68 (Batc (None, 38, 38, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_69 (Batc (None, 38, 38, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_60[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_63[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_68[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_69[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 38, 38, 768)  0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_72 (Batc (None, 38, 38, 192)  576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_72[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 38, 38, 192)  258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_73 (Batc (None, 38, 38, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_73[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 38, 38, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_70 (Batc (None, 38, 38, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_74 (Batc (None, 38, 38, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_70[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_74[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 18, 18, 320)  552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 18, 18, 192)  331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_71 (Batc (None, 18, 18, 320)  960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_75 (Batc (None, 18, 18, 192)  576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_71[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_75[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 18, 18, 768)  0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 18, 18, 1280) 0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 18, 18, 448)  573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_80 (Batc (None, 18, 18, 448)  1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_80[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 18, 18, 384)  491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 18, 18, 384)  1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_77 (Batc (None, 18, 18, 384)  1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_81 (Batc (None, 18, 18, 384)  1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_77[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_81[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 18, 18, 1280) 0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 18, 18, 320)  409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_78 (Batc (None, 18, 18, 384)  1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_79 (Batc (None, 18, 18, 384)  1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_82 (Batc (None, 18, 18, 384)  1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_83 (Batc (None, 18, 18, 384)  1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 18, 18, 192)  245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_76 (Batc (None, 18, 18, 320)  960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_78[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_79[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_82[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_83[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_84 (Batc (None, 18, 18, 192)  576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_76[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 18, 18, 768)  0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 18, 18, 768)  0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_84[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 18, 18, 2048) 0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 18, 18, 448)  917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_89 (Batc (None, 18, 18, 448)  1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_89[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 18, 18, 384)  786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 18, 18, 384)  1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_86 (Batc (None, 18, 18, 384)  1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_90 (Batc (None, 18, 18, 384)  1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_86[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_90[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 18, 18, 2048) 0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 18, 18, 320)  655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_87 (Batc (None, 18, 18, 384)  1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_88 (Batc (None, 18, 18, 384)  1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_91 (Batc (None, 18, 18, 384)  1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_92 (Batc (None, 18, 18, 384)  1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 18, 18, 192)  393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_85 (Batc (None, 18, 18, 320)  960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_87[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_88[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_91[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_92[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_93 (Batc (None, 18, 18, 192)  576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_85[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 18, 18, 768)  0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 18, 18, 768)  0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_93[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 18, 18, 2048) 0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 0\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUc1ad5qGnMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add a classification head\n",
        "#adding a few layers on top of the base model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  keras.layers.GlobalAveragePooling2D(),\n",
        "  keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "#relatively basic layer. no deep neural networks (all happening beforehand). Similar to a logit regression. plain classification\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbhLA3TGr1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "b20438d9-a5cd-4cdb-d4a7-989e9c40c9a5"
      },
      "source": [
        "#Compile the model (Must do before training!)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 18, 18, 2048)      21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 21,804,833\n",
            "Trainable params: 2,049\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th655DVEGwop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8a95dc98-a5d1-4683-f993-dec27e2dc5bd"
      },
      "source": [
        "#These 2K trainable parameters are divided among 2 TensorFlow Variable objects, the weights and biases of the two dense layers:\n",
        "\n",
        "\n",
        "len(model.trainable_variables)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZeMfEwoG3wm",
        "colab_type": "text"
      },
      "source": [
        "**Train the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNgpgcJjG8xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10810
        },
        "outputId": "a380680b-653e-464e-9340-6bcfe35b9c66"
      },
      "source": [
        "epochs = 200\n",
        "steps_per_epoch = train_generator.n // batch_size\n",
        "validation_steps = validation_generator.n // batch_size\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch = steps_per_epoch,\n",
        "                              epochs=epochs,\n",
        "                              workers=4,\n",
        "                              validation_data=validation_generator,\n",
        "                              validation_steps=validation_steps)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0604 16:41:02.149357 140700095268736 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 40s 40s/step - loss: 0.7224 - acc: 0.4500\n",
            "4/4 [==============================] - 116s 29s/step - loss: 0.7130 - acc: 0.4450 - val_loss: 0.7224 - val_acc: 0.4500\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7212 - acc: 0.4300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.7069 - acc: 0.4675 - val_loss: 0.7212 - val_acc: 0.4300\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7213 - acc: 0.4000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.7029 - acc: 0.4800 - val_loss: 0.7213 - val_acc: 0.4000\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7207 - acc: 0.4200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.7008 - acc: 0.5125 - val_loss: 0.7207 - val_acc: 0.4200\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7181 - acc: 0.4000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.7008 - acc: 0.4550 - val_loss: 0.7181 - val_acc: 0.4000\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7165 - acc: 0.4200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6974 - acc: 0.4950 - val_loss: 0.7165 - val_acc: 0.4200\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7160 - acc: 0.4300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6966 - acc: 0.5175 - val_loss: 0.7160 - val_acc: 0.4300\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7136 - acc: 0.4300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6959 - acc: 0.5125 - val_loss: 0.7136 - val_acc: 0.4300\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7119 - acc: 0.4300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6927 - acc: 0.5475 - val_loss: 0.7119 - val_acc: 0.4300\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7115 - acc: 0.4500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6932 - acc: 0.4975 - val_loss: 0.7115 - val_acc: 0.4500\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7094 - acc: 0.4600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6907 - acc: 0.5250 - val_loss: 0.7094 - val_acc: 0.4600\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7090 - acc: 0.4900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6906 - acc: 0.5425 - val_loss: 0.7090 - val_acc: 0.4900\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7067 - acc: 0.4900\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6882 - acc: 0.5275 - val_loss: 0.7067 - val_acc: 0.4900\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7048 - acc: 0.4900\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6876 - acc: 0.5450 - val_loss: 0.7048 - val_acc: 0.4900\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7039 - acc: 0.5200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6889 - acc: 0.5500 - val_loss: 0.7039 - val_acc: 0.5200\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7009 - acc: 0.4800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6854 - acc: 0.5450 - val_loss: 0.7009 - val_acc: 0.4800\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6998 - acc: 0.5200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6830 - acc: 0.5800 - val_loss: 0.6998 - val_acc: 0.5200\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6988 - acc: 0.5400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6822 - acc: 0.5700 - val_loss: 0.6988 - val_acc: 0.5400\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6964 - acc: 0.5600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6800 - acc: 0.5675 - val_loss: 0.6964 - val_acc: 0.5600\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6951 - acc: 0.5600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6801 - acc: 0.5700 - val_loss: 0.6951 - val_acc: 0.5600\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6920 - acc: 0.5400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6779 - acc: 0.5625 - val_loss: 0.6920 - val_acc: 0.5400\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6903 - acc: 0.5300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6765 - acc: 0.5800 - val_loss: 0.6903 - val_acc: 0.5300\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6888 - acc: 0.5300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6752 - acc: 0.6050 - val_loss: 0.6888 - val_acc: 0.5300\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6883 - acc: 0.5700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6727 - acc: 0.6125 - val_loss: 0.6883 - val_acc: 0.5700\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6867 - acc: 0.5700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6722 - acc: 0.6000 - val_loss: 0.6867 - val_acc: 0.5700\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6844 - acc: 0.5500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6704 - acc: 0.6100 - val_loss: 0.6844 - val_acc: 0.5500\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6838 - acc: 0.5700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6689 - acc: 0.6250 - val_loss: 0.6838 - val_acc: 0.5700\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6832 - acc: 0.5600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6697 - acc: 0.6275 - val_loss: 0.6832 - val_acc: 0.5600\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6809 - acc: 0.5700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6677 - acc: 0.6200 - val_loss: 0.6809 - val_acc: 0.5700\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6798 - acc: 0.5600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6661 - acc: 0.6350 - val_loss: 0.6798 - val_acc: 0.5600\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6775 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6655 - acc: 0.6325 - val_loss: 0.6775 - val_acc: 0.5900\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6774 - acc: 0.5800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6629 - acc: 0.6475 - val_loss: 0.6774 - val_acc: 0.5800\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6754 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6612 - acc: 0.6425 - val_loss: 0.6754 - val_acc: 0.5900\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6749 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6624 - acc: 0.6525 - val_loss: 0.6749 - val_acc: 0.6100\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6738 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6601 - acc: 0.6450 - val_loss: 0.6738 - val_acc: 0.5900\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6732 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6598 - acc: 0.6500 - val_loss: 0.6732 - val_acc: 0.6000\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6728 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6612 - acc: 0.6325 - val_loss: 0.6728 - val_acc: 0.6100\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6719 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6564 - acc: 0.6600 - val_loss: 0.6719 - val_acc: 0.6100\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6711 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6558 - acc: 0.6625 - val_loss: 0.6711 - val_acc: 0.6100\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6709 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6556 - acc: 0.6625 - val_loss: 0.6709 - val_acc: 0.6100\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6706 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6546 - acc: 0.6725 - val_loss: 0.6706 - val_acc: 0.6000\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6680 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6534 - acc: 0.6450 - val_loss: 0.6680 - val_acc: 0.5900\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6676 - acc: 0.5800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6513 - acc: 0.6725 - val_loss: 0.6676 - val_acc: 0.5800\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6666 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6532 - acc: 0.6875 - val_loss: 0.6666 - val_acc: 0.5900\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6663 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6524 - acc: 0.6850 - val_loss: 0.6663 - val_acc: 0.5900\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6659 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6490 - acc: 0.6800 - val_loss: 0.6659 - val_acc: 0.6000\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6648 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6493 - acc: 0.6725 - val_loss: 0.6648 - val_acc: 0.6200\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6640 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6474 - acc: 0.6850 - val_loss: 0.6640 - val_acc: 0.6200\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6636 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6453 - acc: 0.6925 - val_loss: 0.6636 - val_acc: 0.6300\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6627 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6467 - acc: 0.7000 - val_loss: 0.6627 - val_acc: 0.6400\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6631 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6508 - acc: 0.6675 - val_loss: 0.6631 - val_acc: 0.6300\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6618 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6431 - acc: 0.6875 - val_loss: 0.6618 - val_acc: 0.6400\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6616 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6430 - acc: 0.7075 - val_loss: 0.6616 - val_acc: 0.6300\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6606 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6457 - acc: 0.6975 - val_loss: 0.6606 - val_acc: 0.6400\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6597 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6427 - acc: 0.6900 - val_loss: 0.6597 - val_acc: 0.6500\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6593 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6415 - acc: 0.6975 - val_loss: 0.6593 - val_acc: 0.6500\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6583 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6406 - acc: 0.6975 - val_loss: 0.6583 - val_acc: 0.6400\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6577 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6389 - acc: 0.7050 - val_loss: 0.6577 - val_acc: 0.6600\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6577 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6379 - acc: 0.7250 - val_loss: 0.6577 - val_acc: 0.6400\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6559 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6404 - acc: 0.6950 - val_loss: 0.6559 - val_acc: 0.6200\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6554 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6388 - acc: 0.7050 - val_loss: 0.6554 - val_acc: 0.6200\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6548 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6349 - acc: 0.7150 - val_loss: 0.6548 - val_acc: 0.6300\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6544 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6346 - acc: 0.7200 - val_loss: 0.6544 - val_acc: 0.6300\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6532 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6338 - acc: 0.7125 - val_loss: 0.6532 - val_acc: 0.6200\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6529 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6357 - acc: 0.7025 - val_loss: 0.6529 - val_acc: 0.6300\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6522 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6357 - acc: 0.6900 - val_loss: 0.6522 - val_acc: 0.6300\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6524 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6327 - acc: 0.7275 - val_loss: 0.6524 - val_acc: 0.6500\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6515 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6309 - acc: 0.7200 - val_loss: 0.6515 - val_acc: 0.6300\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6505 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6310 - acc: 0.7125 - val_loss: 0.6505 - val_acc: 0.6300\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6494 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6296 - acc: 0.7350 - val_loss: 0.6494 - val_acc: 0.6300\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6484 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6303 - acc: 0.7175 - val_loss: 0.6484 - val_acc: 0.6500\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6478 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6296 - acc: 0.7100 - val_loss: 0.6478 - val_acc: 0.6600\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6477 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6262 - acc: 0.7250 - val_loss: 0.6477 - val_acc: 0.6100\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6472 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6260 - acc: 0.7300 - val_loss: 0.6472 - val_acc: 0.6100\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6468 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6271 - acc: 0.7250 - val_loss: 0.6468 - val_acc: 0.6300\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6463 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6246 - acc: 0.7225 - val_loss: 0.6463 - val_acc: 0.6300\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6447 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6236 - acc: 0.7250 - val_loss: 0.6447 - val_acc: 0.6500\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6439 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6224 - acc: 0.7100 - val_loss: 0.6439 - val_acc: 0.6600\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6431 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6195 - acc: 0.7300 - val_loss: 0.6431 - val_acc: 0.6700\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6427 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6242 - acc: 0.7225 - val_loss: 0.6427 - val_acc: 0.6500\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6424 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6226 - acc: 0.7200 - val_loss: 0.6424 - val_acc: 0.6300\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6423 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6186 - acc: 0.7475 - val_loss: 0.6423 - val_acc: 0.6200\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6418 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6194 - acc: 0.7200 - val_loss: 0.6418 - val_acc: 0.6200\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6406 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6212 - acc: 0.7225 - val_loss: 0.6406 - val_acc: 0.6400\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6405 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6158 - acc: 0.7475 - val_loss: 0.6405 - val_acc: 0.6300\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6394 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6212 - acc: 0.7250 - val_loss: 0.6394 - val_acc: 0.6400\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6387 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6151 - acc: 0.7425 - val_loss: 0.6387 - val_acc: 0.6600\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6379 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6139 - acc: 0.7450 - val_loss: 0.6379 - val_acc: 0.6700\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6381 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6148 - acc: 0.7450 - val_loss: 0.6381 - val_acc: 0.6400\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6373 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6114 - acc: 0.7450 - val_loss: 0.6373 - val_acc: 0.6500\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6376 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6117 - acc: 0.7400 - val_loss: 0.6376 - val_acc: 0.6100\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6372 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6135 - acc: 0.7350 - val_loss: 0.6372 - val_acc: 0.6200\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6362 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6138 - acc: 0.7350 - val_loss: 0.6362 - val_acc: 0.6300\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6359 - acc: 0.6300\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6120 - acc: 0.7350 - val_loss: 0.6359 - val_acc: 0.6300\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6353 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6111 - acc: 0.7400 - val_loss: 0.6353 - val_acc: 0.6300\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6343 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6121 - acc: 0.7250 - val_loss: 0.6343 - val_acc: 0.6600\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6342 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6107 - acc: 0.7275 - val_loss: 0.6342 - val_acc: 0.6300\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6341 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6092 - acc: 0.7475 - val_loss: 0.6341 - val_acc: 0.6300\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6327 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6066 - acc: 0.7500 - val_loss: 0.6327 - val_acc: 0.6800\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6334 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6047 - acc: 0.7500 - val_loss: 0.6334 - val_acc: 0.6300\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6321 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6057 - acc: 0.7575 - val_loss: 0.6321 - val_acc: 0.6500\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6308 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6045 - acc: 0.7625 - val_loss: 0.6308 - val_acc: 0.6600\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6307 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6055 - acc: 0.7675 - val_loss: 0.6307 - val_acc: 0.6800\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6297 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6057 - acc: 0.7300 - val_loss: 0.6297 - val_acc: 0.6600\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6301 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6049 - acc: 0.7375 - val_loss: 0.6301 - val_acc: 0.6400\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6296 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6009 - acc: 0.7650 - val_loss: 0.6296 - val_acc: 0.6400\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6289 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6046 - acc: 0.7450 - val_loss: 0.6289 - val_acc: 0.6700\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6286 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6012 - acc: 0.7500 - val_loss: 0.6286 - val_acc: 0.6500\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6274 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5999 - acc: 0.7550 - val_loss: 0.6274 - val_acc: 0.6800\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6269 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5999 - acc: 0.7575 - val_loss: 0.6269 - val_acc: 0.6800\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6276 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5997 - acc: 0.7600 - val_loss: 0.6276 - val_acc: 0.6300\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6261 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5995 - acc: 0.7550 - val_loss: 0.6261 - val_acc: 0.6800\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6266 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5985 - acc: 0.7700 - val_loss: 0.6266 - val_acc: 0.6600\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6269 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5976 - acc: 0.7550 - val_loss: 0.6269 - val_acc: 0.6300\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6250 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5964 - acc: 0.7650 - val_loss: 0.6250 - val_acc: 0.6800\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6249 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5968 - acc: 0.7650 - val_loss: 0.6249 - val_acc: 0.6800\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6252 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5950 - acc: 0.7600 - val_loss: 0.6252 - val_acc: 0.6400\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6249 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5969 - acc: 0.7525 - val_loss: 0.6249 - val_acc: 0.6400\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6247 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5941 - acc: 0.7575 - val_loss: 0.6247 - val_acc: 0.6300\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6241 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5938 - acc: 0.7600 - val_loss: 0.6241 - val_acc: 0.6400\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6239 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5957 - acc: 0.7475 - val_loss: 0.6239 - val_acc: 0.6400\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6229 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5916 - acc: 0.7625 - val_loss: 0.6229 - val_acc: 0.6500\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6217 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5900 - acc: 0.7700 - val_loss: 0.6217 - val_acc: 0.6800\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6224 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5900 - acc: 0.7650 - val_loss: 0.6224 - val_acc: 0.6500\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6205 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5917 - acc: 0.7500 - val_loss: 0.6205 - val_acc: 0.6800\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6203 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5923 - acc: 0.7625 - val_loss: 0.6203 - val_acc: 0.6800\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6204 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5897 - acc: 0.7525 - val_loss: 0.6204 - val_acc: 0.6800\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6202 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5889 - acc: 0.7500 - val_loss: 0.6202 - val_acc: 0.6700\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6191 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5881 - acc: 0.7700 - val_loss: 0.6191 - val_acc: 0.6800\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6189 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5873 - acc: 0.7500 - val_loss: 0.6189 - val_acc: 0.6800\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6185 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5866 - acc: 0.7775 - val_loss: 0.6185 - val_acc: 0.6800\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6191 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5887 - acc: 0.7650 - val_loss: 0.6191 - val_acc: 0.6500\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6180 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5875 - acc: 0.7575 - val_loss: 0.6180 - val_acc: 0.6700\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6174 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5874 - acc: 0.7700 - val_loss: 0.6174 - val_acc: 0.6800\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6171 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5848 - acc: 0.7625 - val_loss: 0.6171 - val_acc: 0.6800\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6176 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5855 - acc: 0.7750 - val_loss: 0.6176 - val_acc: 0.6500\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6167 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5826 - acc: 0.7825 - val_loss: 0.6167 - val_acc: 0.6700\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6174 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5842 - acc: 0.7675 - val_loss: 0.6174 - val_acc: 0.6500\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6162 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5828 - acc: 0.7675 - val_loss: 0.6162 - val_acc: 0.6700\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6154 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5830 - acc: 0.7675 - val_loss: 0.6154 - val_acc: 0.6700\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6149 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5804 - acc: 0.7700 - val_loss: 0.6149 - val_acc: 0.6800\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6149 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5799 - acc: 0.7825 - val_loss: 0.6149 - val_acc: 0.6700\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6139 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5792 - acc: 0.7800 - val_loss: 0.6139 - val_acc: 0.6800\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6134 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5778 - acc: 0.7775 - val_loss: 0.6134 - val_acc: 0.6800\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6131 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5794 - acc: 0.7725 - val_loss: 0.6131 - val_acc: 0.6800\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6124 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5806 - acc: 0.7625 - val_loss: 0.6124 - val_acc: 0.6800\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6135 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5797 - acc: 0.7625 - val_loss: 0.6135 - val_acc: 0.6500\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6133 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5827 - acc: 0.7600 - val_loss: 0.6133 - val_acc: 0.6500\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6136 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5788 - acc: 0.7675 - val_loss: 0.6136 - val_acc: 0.6500\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6119 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5799 - acc: 0.7600 - val_loss: 0.6119 - val_acc: 0.6700\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6110 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5769 - acc: 0.7625 - val_loss: 0.6110 - val_acc: 0.6800\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6112 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5741 - acc: 0.7825 - val_loss: 0.6112 - val_acc: 0.6800\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6115 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5767 - acc: 0.7800 - val_loss: 0.6115 - val_acc: 0.6700\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6109 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5737 - acc: 0.7725 - val_loss: 0.6109 - val_acc: 0.6700\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6102 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5741 - acc: 0.7700 - val_loss: 0.6102 - val_acc: 0.6800\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6103 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5726 - acc: 0.7900 - val_loss: 0.6103 - val_acc: 0.6700\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6098 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5714 - acc: 0.7800 - val_loss: 0.6098 - val_acc: 0.6700\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6090 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5731 - acc: 0.7825 - val_loss: 0.6090 - val_acc: 0.6800\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6087 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5722 - acc: 0.7750 - val_loss: 0.6087 - val_acc: 0.6800\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6095 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5703 - acc: 0.7775 - val_loss: 0.6095 - val_acc: 0.6500\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6093 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5690 - acc: 0.7875 - val_loss: 0.6093 - val_acc: 0.6500\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6080 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5703 - acc: 0.7800 - val_loss: 0.6080 - val_acc: 0.6700\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6072 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5675 - acc: 0.7850 - val_loss: 0.6072 - val_acc: 0.6800\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6070 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5683 - acc: 0.7775 - val_loss: 0.6070 - val_acc: 0.6700\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6071 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5676 - acc: 0.7775 - val_loss: 0.6071 - val_acc: 0.6700\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6073 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5671 - acc: 0.7775 - val_loss: 0.6073 - val_acc: 0.6600\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6072 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5687 - acc: 0.7825 - val_loss: 0.6072 - val_acc: 0.6500\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6059 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5679 - acc: 0.7600 - val_loss: 0.6059 - val_acc: 0.6700\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6063 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5647 - acc: 0.7875 - val_loss: 0.6063 - val_acc: 0.6600\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6059 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5653 - acc: 0.7875 - val_loss: 0.6059 - val_acc: 0.6600\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6059 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5643 - acc: 0.8000 - val_loss: 0.6059 - val_acc: 0.6600\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6066 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5652 - acc: 0.7750 - val_loss: 0.6066 - val_acc: 0.6500\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6052 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5661 - acc: 0.7825 - val_loss: 0.6052 - val_acc: 0.6600\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6044 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5641 - acc: 0.7800 - val_loss: 0.6044 - val_acc: 0.6700\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6032 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5648 - acc: 0.7850 - val_loss: 0.6032 - val_acc: 0.6800\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6034 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5649 - acc: 0.7700 - val_loss: 0.6034 - val_acc: 0.6800\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6042 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5630 - acc: 0.7800 - val_loss: 0.6042 - val_acc: 0.6600\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6044 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5631 - acc: 0.7950 - val_loss: 0.6044 - val_acc: 0.6500\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6033 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5593 - acc: 0.7925 - val_loss: 0.6033 - val_acc: 0.6600\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6022 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5594 - acc: 0.7925 - val_loss: 0.6022 - val_acc: 0.6800\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6019 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5618 - acc: 0.7925 - val_loss: 0.6019 - val_acc: 0.6800\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6022 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5589 - acc: 0.7950 - val_loss: 0.6022 - val_acc: 0.6700\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6014 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5570 - acc: 0.7850 - val_loss: 0.6014 - val_acc: 0.6800\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6009 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5631 - acc: 0.7750 - val_loss: 0.6009 - val_acc: 0.6800\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6027 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5581 - acc: 0.7900 - val_loss: 0.6027 - val_acc: 0.6600\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6015 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5650 - acc: 0.7625 - val_loss: 0.6015 - val_acc: 0.6700\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6006 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5569 - acc: 0.7900 - val_loss: 0.6006 - val_acc: 0.6800\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5997 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5612 - acc: 0.7700 - val_loss: 0.5997 - val_acc: 0.6700\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5997 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5567 - acc: 0.7950 - val_loss: 0.5997 - val_acc: 0.6700\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5996 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5573 - acc: 0.7875 - val_loss: 0.5996 - val_acc: 0.6700\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6006 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5542 - acc: 0.7975 - val_loss: 0.6006 - val_acc: 0.6600\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6002 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5575 - acc: 0.7925 - val_loss: 0.6002 - val_acc: 0.6700\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5997 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5547 - acc: 0.7900 - val_loss: 0.5997 - val_acc: 0.6700\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5988 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5566 - acc: 0.7825 - val_loss: 0.5988 - val_acc: 0.6800\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5989 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5556 - acc: 0.7825 - val_loss: 0.5989 - val_acc: 0.6800\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5999 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5529 - acc: 0.7900 - val_loss: 0.5999 - val_acc: 0.6600\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5986 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5512 - acc: 0.8075 - val_loss: 0.5986 - val_acc: 0.6800\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5976 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5549 - acc: 0.7775 - val_loss: 0.5976 - val_acc: 0.6700\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5980 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5520 - acc: 0.8000 - val_loss: 0.5980 - val_acc: 0.6800\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5983 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5561 - acc: 0.7950 - val_loss: 0.5983 - val_acc: 0.6600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgtyZLsvG_ls",
        "colab_type": "text"
      },
      "source": [
        "**Learning curves**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-ketbynHDhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "9167adab-ac9d-4d40-8337-d37cd0e88b70"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "#get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy - 3')\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.title('Training and Validation Loss - 3')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngPKdyzuvTVUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqRtjjDFxwpK6McYYEycsqRtjjDFx4v8BZmzuCyFjnggAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POHLQd9PPMLX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0fc08b11-c03d-4527-9a4c-13579e5a224e"
      },
      "source": [
        "_, acc = model.evaluate_generator(generator=train_generator)\n",
        "print('Accuracy on training set: {}%'.format(round(acc, 3)*100))\n",
        "\n",
        "_, acc = model.evaluate_generator(generator=validation_generator)\n",
        "print('Accuracy on validation set: {}%'.format(round(acc, 3)*100))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 79.79999780654907%\n",
            "Accuracy on validation set: 66.00000262260437%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeN1ZqTAM42i",
        "colab_type": "text"
      },
      "source": [
        "**Export Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkUfyxqDM3NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keras library import  for Saving and loading model and weights\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwvXWQnGEENs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = model.to_json()\n",
        "\n",
        "\n",
        "with open(\"drive/Colab Notebooks/smodel/s3_model_num.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"drive/Colab Notebooks/smodel/s3_model_num.h5\")\n",
        "model.save('drive/Colab Notebooks/smodel/s3_model_num.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}