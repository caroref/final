{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S2 Housing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caroref/final/blob/master/S2_Housing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ascUYWYXFlTS",
        "colab_type": "code",
        "outputId": "9285392e-6ab6-4e68-c71a-d07fbd0845c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TensorFlow version is \", tf.__version__)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0604 15:03:24.246975 139833862203264 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version is  1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs-4SeS3Fpc5",
        "colab_type": "code",
        "outputId": "52f07c40-e1f3-4bc9-b62e-0a25d5bd2bac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Allow Colab to access drive\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "#linking Colab to use drive as if local\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDNLdWVxP2jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main directories\n",
        "base_dir = 'drive/Colab/EV/Base/'\n",
        "like = os.path.join(base_dir, 'Like')\n",
        "dislike = os.path.join(base_dir, 'Dislike')\n",
        "\n",
        "trn_dir = 'drive/Colab/EV/S2/Train/'\n",
        "tst_dir = 'drive/Colab/EV/S2/Test/'\n",
        "\n",
        "#subdirectories\n",
        "trn_like = os.path.join(trn_dir, 'Like')\n",
        "tst_like = os.path.join(tst_dir, 'Like')\n",
        "\n",
        "trn_disl = os.path.join(trn_dir, 'Dislike')\n",
        "tst_disl = os.path.join(tst_dir, 'Dislike')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo3GVjj1FsEn",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5BtzOSmbZB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All images will be resized to image_size\n",
        "\n",
        "image_size = 640 \n",
        "batch_size = 100\n",
        "\n",
        "# create Image Data Generator for Image Data Augmentation\n",
        "\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "                rescale=1./255, \n",
        "                validation_split=0.2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTvB7EieGDAN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1a68b873-4e0a-49f9-992c-2ff3a1f5508a"
      },
      "source": [
        "# Create the train and validation generators and specify the train dataset directory, \n",
        "# image size, batch size, binary classification, and subset (training or validation).\n",
        "\n",
        "\n",
        "# Flow training images in batches using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,  \n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode ='binary',\n",
        "                subset = 'training') "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ58vNBzRz0J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e39114f-58ba-4e42-d9e9-255ab7b57059"
      },
      "source": [
        "# Flow validation images in batches using validation_datagen generator\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,\n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode='binary',\n",
        "                subset = 'validation')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ur5r8uGWQO",
        "colab_type": "text"
      },
      "source": [
        "# Create the base model from the pre-trained convnets\n",
        "\n",
        "Using the output of last layer (bottleneck layer). Keep other layers frozen (these are specific to their training, less useful here?) Bottleneck layer should be more general.\n",
        "\n",
        "Using Inception v3 model with weights trained on ImageNet.\n",
        "The argument, include_top=False, means that we have a network that isn't including classification layers at the top. This is good for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCLmagoGbc3",
        "colab_type": "code",
        "outputId": "bc75808f-d9cb-4ea9-b77e-8f221136291f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "IMG_SHAPE = (image_size, image_size, 3)\n",
        "\n",
        "base_model = keras.applications.inception_v3.InceptionV3(\n",
        "             include_top=False, weights='imagenet', input_shape=IMG_SHAPE)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0604 15:04:22.232720 139833862203264 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wXzV2zlGduw",
        "colab_type": "text"
      },
      "source": [
        "# Feature extraction\n",
        "We will freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "It's important to freeze the convolutional based before we compile and train the model. By freezing (or setting layer.trainable = False), we prevent the weights in these layers from being updated during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCk3KhmGGhaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11821
        },
        "outputId": "9d7f52cc-0734-4623-f7aa-5e8e152c4b1a"
      },
      "source": [
        "#freeze base\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# Look at the base model architecture\n",
        "base_model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 640, 640, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 319, 319, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 319, 319, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 319, 319, 32) 0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 317, 317, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 317, 317, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 317, 317, 32) 0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 317, 317, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 317, 317, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 317, 317, 64) 0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 158, 158, 64) 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 158, 158, 80) 5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 158, 158, 80) 240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 158, 158, 80) 0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 156, 156, 192 138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 156, 156, 192 576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 156, 156, 192 0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 77, 77, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_8 (Batch (None, 77, 77, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 77, 77, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 77, 77, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_6 (Batch (None, 77, 77, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_9 (Batch (None, 77, 77, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 77, 77, 48)   0           batch_normalization_v1_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 77, 77, 96)   0           batch_normalization_v1_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 77, 77, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 77, 77, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 77, 77, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 77, 77, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_5 (Batch (None, 77, 77, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_7 (Batch (None, 77, 77, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_10 (Batc (None, 77, 77, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_11 (Batc (None, 77, 77, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 77, 77, 32)   0           batch_normalization_v1_11[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 77, 77, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_15 (Batc (None, 77, 77, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_15[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 77, 77, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 77, 77, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_13 (Batc (None, 77, 77, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_16 (Batc (None, 77, 77, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_13[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_16[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 77, 77, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 77, 77, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 77, 77, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 77, 77, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_12 (Batc (None, 77, 77, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_14 (Batc (None, 77, 77, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_17 (Batc (None, 77, 77, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_18 (Batc (None, 77, 77, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_12[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_14[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_17[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_18[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 77, 77, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_22 (Batc (None, 77, 77, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_22[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 77, 77, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 77, 77, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_20 (Batc (None, 77, 77, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_23 (Batc (None, 77, 77, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_20[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_23[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 77, 77, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 77, 77, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 77, 77, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 77, 77, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_19 (Batc (None, 77, 77, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_21 (Batc (None, 77, 77, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_24 (Batc (None, 77, 77, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_25 (Batc (None, 77, 77, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_19[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_21[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_24[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_25[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 77, 77, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 77, 77, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_27 (Batc (None, 77, 77, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_27[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 77, 77, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_28 (Batc (None, 77, 77, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_28[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 38, 38, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 38, 38, 96)   82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_26 (Batc (None, 38, 38, 384)  1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_29 (Batc (None, 38, 38, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 38, 38, 384)  0           batch_normalization_v1_26[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 38, 38, 96)   0           batch_normalization_v1_29[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 38, 38, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 38, 38, 768)  0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_34 (Batc (None, 38, 38, 128)  384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_34[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 38, 38, 128)  114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_35 (Batc (None, 38, 38, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_35[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 38, 38, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_31 (Batc (None, 38, 38, 128)  384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_36 (Batc (None, 38, 38, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_31[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_36[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 38, 38, 128)  114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 38, 38, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_32 (Batc (None, 38, 38, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_37 (Batc (None, 38, 38, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_32[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_37[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 38, 38, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 38, 38, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 38, 38, 192)  172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 38, 38, 192)  172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_30 (Batc (None, 38, 38, 192)  576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_33 (Batc (None, 38, 38, 192)  576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_38 (Batc (None, 38, 38, 192)  576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_39 (Batc (None, 38, 38, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_30[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_33[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_38[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_39[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 38, 38, 768)  0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_44 (Batc (None, 38, 38, 160)  480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_44[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 38, 38, 160)  179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_45 (Batc (None, 38, 38, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_45[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 38, 38, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_41 (Batc (None, 38, 38, 160)  480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_46 (Batc (None, 38, 38, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_41[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_46[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 38, 38, 160)  179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 38, 38, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_42 (Batc (None, 38, 38, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_47 (Batc (None, 38, 38, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_42[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_47[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 38, 38, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 38, 38, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 38, 38, 192)  215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 38, 38, 192)  215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_40 (Batc (None, 38, 38, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_43 (Batc (None, 38, 38, 192)  576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_48 (Batc (None, 38, 38, 192)  576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_49 (Batc (None, 38, 38, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_40[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_43[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_48[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_49[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 38, 38, 768)  0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_54 (Batc (None, 38, 38, 160)  480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_54[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 38, 38, 160)  179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_55 (Batc (None, 38, 38, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_55[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 38, 38, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_51 (Batc (None, 38, 38, 160)  480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_56 (Batc (None, 38, 38, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_51[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_56[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 38, 38, 160)  179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 38, 38, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_52 (Batc (None, 38, 38, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_57 (Batc (None, 38, 38, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_52[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_57[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 38, 38, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 38, 38, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 38, 38, 192)  215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 38, 38, 192)  215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_50 (Batc (None, 38, 38, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_53 (Batc (None, 38, 38, 192)  576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_58 (Batc (None, 38, 38, 192)  576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_59 (Batc (None, 38, 38, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_50[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_53[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_58[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_59[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 38, 38, 768)  0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_64 (Batc (None, 38, 38, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_64[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 38, 38, 192)  258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_65 (Batc (None, 38, 38, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_65[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 38, 38, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_61 (Batc (None, 38, 38, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_66 (Batc (None, 38, 38, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_61[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_66[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 38, 38, 192)  258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 38, 38, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_62 (Batc (None, 38, 38, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_67 (Batc (None, 38, 38, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_62[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_67[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 38, 38, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 38, 38, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 38, 38, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_60 (Batc (None, 38, 38, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_63 (Batc (None, 38, 38, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_68 (Batc (None, 38, 38, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_69 (Batc (None, 38, 38, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_60[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_63[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_68[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_69[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 38, 38, 768)  0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_72 (Batc (None, 38, 38, 192)  576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_72[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 38, 38, 192)  258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_73 (Batc (None, 38, 38, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_73[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 38, 38, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_70 (Batc (None, 38, 38, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_74 (Batc (None, 38, 38, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_70[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_74[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 18, 18, 320)  552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 18, 18, 192)  331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_71 (Batc (None, 18, 18, 320)  960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_75 (Batc (None, 18, 18, 192)  576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_71[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_75[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 18, 18, 768)  0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 18, 18, 1280) 0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 18, 18, 448)  573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_80 (Batc (None, 18, 18, 448)  1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_80[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 18, 18, 384)  491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 18, 18, 384)  1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_77 (Batc (None, 18, 18, 384)  1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_81 (Batc (None, 18, 18, 384)  1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_77[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_81[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 18, 18, 1280) 0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 18, 18, 320)  409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_78 (Batc (None, 18, 18, 384)  1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_79 (Batc (None, 18, 18, 384)  1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_82 (Batc (None, 18, 18, 384)  1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_83 (Batc (None, 18, 18, 384)  1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 18, 18, 192)  245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_76 (Batc (None, 18, 18, 320)  960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_78[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_79[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_82[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_83[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_84 (Batc (None, 18, 18, 192)  576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_76[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 18, 18, 768)  0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 18, 18, 768)  0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_84[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 18, 18, 2048) 0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 18, 18, 448)  917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_89 (Batc (None, 18, 18, 448)  1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_89[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 18, 18, 384)  786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 18, 18, 384)  1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_86 (Batc (None, 18, 18, 384)  1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_90 (Batc (None, 18, 18, 384)  1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_86[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_90[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 18, 18, 2048) 0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 18, 18, 320)  655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_87 (Batc (None, 18, 18, 384)  1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_88 (Batc (None, 18, 18, 384)  1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_91 (Batc (None, 18, 18, 384)  1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_92 (Batc (None, 18, 18, 384)  1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 18, 18, 192)  393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_85 (Batc (None, 18, 18, 320)  960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_87[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_88[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_91[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_92[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_93 (Batc (None, 18, 18, 192)  576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_85[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 18, 18, 768)  0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 18, 18, 768)  0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_93[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 18, 18, 2048) 0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 0\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUc1ad5qGnMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add a classification head\n",
        "#adding a few layers on top of the base model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  keras.layers.GlobalAveragePooling2D(),\n",
        "  keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "#relatively basic layer. no deep neural networks (all happening beforehand). Similar to a logit regression. plain classification\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbhLA3TGr1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "590a00be-dcd8-43b9-9c16-791c00f66896"
      },
      "source": [
        "#Compile the model (Must do before training!)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 18, 18, 2048)      21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 21,804,833\n",
            "Trainable params: 2,049\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th655DVEGwop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4f8ad696-fa1c-4783-9cab-337756b70c80"
      },
      "source": [
        "#These 2K trainable parameters are divided among 2 TensorFlow Variable objects, the weights and biases of the two dense layers:\n",
        "\n",
        "\n",
        "len(model.trainable_variables)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZeMfEwoG3wm",
        "colab_type": "text"
      },
      "source": [
        "**Train the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNgpgcJjG8xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10810
        },
        "outputId": "e0559099-1fe8-4f52-e611-b2bff100ea4d"
      },
      "source": [
        "epochs = 200\n",
        "steps_per_epoch = train_generator.n // batch_size\n",
        "validation_steps = validation_generator.n // batch_size\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch = steps_per_epoch,\n",
        "                              epochs=epochs,\n",
        "                              workers=4,\n",
        "                              validation_data=validation_generator,\n",
        "                              validation_steps=validation_steps)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0604 15:04:39.903042 139833862203264 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 57s 57s/step - loss: 0.7231 - acc: 0.4900\n",
            "4/4 [==============================] - 136s 34s/step - loss: 0.7556 - acc: 0.5000 - val_loss: 0.7231 - val_acc: 0.4900\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7138 - acc: 0.4200\n",
            "4/4 [==============================] - 12s 3s/step - loss: 0.7173 - acc: 0.4825 - val_loss: 0.7138 - val_acc: 0.4200\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7108 - acc: 0.4400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.7075 - acc: 0.4750 - val_loss: 0.7108 - val_acc: 0.4400\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7095 - acc: 0.4500\n",
            "4/4 [==============================] - 12s 3s/step - loss: 0.7057 - acc: 0.4675 - val_loss: 0.7095 - val_acc: 0.4500\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7089 - acc: 0.4300\n",
            "4/4 [==============================] - 12s 3s/step - loss: 0.7037 - acc: 0.4775 - val_loss: 0.7089 - val_acc: 0.4300\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7083 - acc: 0.4500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.7011 - acc: 0.4775 - val_loss: 0.7083 - val_acc: 0.4500\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7080 - acc: 0.4500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.7005 - acc: 0.4950 - val_loss: 0.7080 - val_acc: 0.4500\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7075 - acc: 0.4700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.7004 - acc: 0.4600 - val_loss: 0.7075 - val_acc: 0.4700\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7072 - acc: 0.4800\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6996 - acc: 0.4950 - val_loss: 0.7072 - val_acc: 0.4800\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7068 - acc: 0.4900\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6967 - acc: 0.4825 - val_loss: 0.7068 - val_acc: 0.4900\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7063 - acc: 0.5100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6952 - acc: 0.5200 - val_loss: 0.7063 - val_acc: 0.5100\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7057 - acc: 0.5100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6954 - acc: 0.5150 - val_loss: 0.7057 - val_acc: 0.5100\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7051 - acc: 0.5000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6938 - acc: 0.4950 - val_loss: 0.7051 - val_acc: 0.5000\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7044 - acc: 0.5100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6938 - acc: 0.5150 - val_loss: 0.7044 - val_acc: 0.5100\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7038 - acc: 0.5100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6932 - acc: 0.5125 - val_loss: 0.7038 - val_acc: 0.5100\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7029 - acc: 0.4900\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6901 - acc: 0.5350 - val_loss: 0.7029 - val_acc: 0.4900\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7018 - acc: 0.5000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6894 - acc: 0.5400 - val_loss: 0.7018 - val_acc: 0.5000\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7008 - acc: 0.4900\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6873 - acc: 0.5550 - val_loss: 0.7008 - val_acc: 0.4900\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6998 - acc: 0.5000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6870 - acc: 0.5750 - val_loss: 0.6998 - val_acc: 0.5000\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6990 - acc: 0.5200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6864 - acc: 0.5475 - val_loss: 0.6990 - val_acc: 0.5200\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6983 - acc: 0.5400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6853 - acc: 0.5550 - val_loss: 0.6983 - val_acc: 0.5400\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6970 - acc: 0.5100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6834 - acc: 0.5550 - val_loss: 0.6970 - val_acc: 0.5100\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6960 - acc: 0.5000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6818 - acc: 0.5975 - val_loss: 0.6960 - val_acc: 0.5000\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6950 - acc: 0.5300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6811 - acc: 0.6050 - val_loss: 0.6950 - val_acc: 0.5300\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6940 - acc: 0.5100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6801 - acc: 0.5875 - val_loss: 0.6940 - val_acc: 0.5100\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6932 - acc: 0.5300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6760 - acc: 0.6275 - val_loss: 0.6932 - val_acc: 0.5300\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6922 - acc: 0.5100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6773 - acc: 0.6300 - val_loss: 0.6922 - val_acc: 0.5100\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6914 - acc: 0.5100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6769 - acc: 0.6200 - val_loss: 0.6914 - val_acc: 0.5100\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6909 - acc: 0.5500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6768 - acc: 0.6000 - val_loss: 0.6909 - val_acc: 0.5500\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6902 - acc: 0.5400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6745 - acc: 0.6375 - val_loss: 0.6902 - val_acc: 0.5400\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6895 - acc: 0.5500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6716 - acc: 0.6475 - val_loss: 0.6895 - val_acc: 0.5500\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6887 - acc: 0.5400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6718 - acc: 0.6375 - val_loss: 0.6887 - val_acc: 0.5400\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6880 - acc: 0.5600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6718 - acc: 0.6575 - val_loss: 0.6880 - val_acc: 0.5600\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6874 - acc: 0.5500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6722 - acc: 0.6225 - val_loss: 0.6874 - val_acc: 0.5500\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6869 - acc: 0.5600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6680 - acc: 0.6600 - val_loss: 0.6869 - val_acc: 0.5600\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6865 - acc: 0.5700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6706 - acc: 0.6325 - val_loss: 0.6865 - val_acc: 0.5700\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6859 - acc: 0.5700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6651 - acc: 0.6625 - val_loss: 0.6859 - val_acc: 0.5700\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6854 - acc: 0.5700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6669 - acc: 0.6575 - val_loss: 0.6854 - val_acc: 0.5700\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6851 - acc: 0.5800\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6669 - acc: 0.6600 - val_loss: 0.6851 - val_acc: 0.5800\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6846 - acc: 0.5900\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6628 - acc: 0.6725 - val_loss: 0.6846 - val_acc: 0.5900\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6841 - acc: 0.5700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6628 - acc: 0.6650 - val_loss: 0.6841 - val_acc: 0.5700\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6836 - acc: 0.5700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6615 - acc: 0.6700 - val_loss: 0.6836 - val_acc: 0.5700\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6832 - acc: 0.5800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6613 - acc: 0.6825 - val_loss: 0.6832 - val_acc: 0.5800\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6828 - acc: 0.5900\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6616 - acc: 0.6625 - val_loss: 0.6828 - val_acc: 0.5900\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6824 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6624 - acc: 0.6575 - val_loss: 0.6824 - val_acc: 0.6000\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6820 - acc: 0.5900\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6578 - acc: 0.6750 - val_loss: 0.6820 - val_acc: 0.5900\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6817 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6593 - acc: 0.6750 - val_loss: 0.6817 - val_acc: 0.6000\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6810 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6563 - acc: 0.6925 - val_loss: 0.6810 - val_acc: 0.6000\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6804 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6541 - acc: 0.6950 - val_loss: 0.6804 - val_acc: 0.6100\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6797 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6534 - acc: 0.6925 - val_loss: 0.6797 - val_acc: 0.6200\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6791 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6536 - acc: 0.6900 - val_loss: 0.6791 - val_acc: 0.6100\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6783 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6535 - acc: 0.6650 - val_loss: 0.6783 - val_acc: 0.6200\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6779 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6512 - acc: 0.6750 - val_loss: 0.6779 - val_acc: 0.6300\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6771 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6496 - acc: 0.7000 - val_loss: 0.6771 - val_acc: 0.6200\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6765 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6496 - acc: 0.6900 - val_loss: 0.6765 - val_acc: 0.6200\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6760 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6466 - acc: 0.6825 - val_loss: 0.6760 - val_acc: 0.6500\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6754 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6473 - acc: 0.7050 - val_loss: 0.6754 - val_acc: 0.6500\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6747 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6453 - acc: 0.6925 - val_loss: 0.6747 - val_acc: 0.6200\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6744 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6478 - acc: 0.7025 - val_loss: 0.6744 - val_acc: 0.6100\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6738 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6433 - acc: 0.6850 - val_loss: 0.6738 - val_acc: 0.6200\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6735 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6423 - acc: 0.7025 - val_loss: 0.6735 - val_acc: 0.6600\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6731 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6438 - acc: 0.7100 - val_loss: 0.6731 - val_acc: 0.6500\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6727 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6393 - acc: 0.6950 - val_loss: 0.6727 - val_acc: 0.6300\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6724 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6424 - acc: 0.6950 - val_loss: 0.6724 - val_acc: 0.6300\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6721 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6387 - acc: 0.7100 - val_loss: 0.6721 - val_acc: 0.6500\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6716 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6365 - acc: 0.7050 - val_loss: 0.6716 - val_acc: 0.6400\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6713 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6364 - acc: 0.7100 - val_loss: 0.6713 - val_acc: 0.6600\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6708 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6386 - acc: 0.6950 - val_loss: 0.6708 - val_acc: 0.6400\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6705 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6371 - acc: 0.6950 - val_loss: 0.6705 - val_acc: 0.6200\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6701 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6344 - acc: 0.7150 - val_loss: 0.6701 - val_acc: 0.6200\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6697 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6359 - acc: 0.6925 - val_loss: 0.6697 - val_acc: 0.6500\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6694 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6327 - acc: 0.7250 - val_loss: 0.6694 - val_acc: 0.6700\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6688 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6327 - acc: 0.7225 - val_loss: 0.6688 - val_acc: 0.6300\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6684 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6309 - acc: 0.7175 - val_loss: 0.6684 - val_acc: 0.6300\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6680 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6296 - acc: 0.7300 - val_loss: 0.6680 - val_acc: 0.6400\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6678 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6305 - acc: 0.7125 - val_loss: 0.6678 - val_acc: 0.6600\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6672 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6284 - acc: 0.7275 - val_loss: 0.6672 - val_acc: 0.6500\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6669 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6312 - acc: 0.7075 - val_loss: 0.6669 - val_acc: 0.6400\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6666 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6258 - acc: 0.7225 - val_loss: 0.6666 - val_acc: 0.6300\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6663 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6269 - acc: 0.7325 - val_loss: 0.6663 - val_acc: 0.6400\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6660 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6251 - acc: 0.7200 - val_loss: 0.6660 - val_acc: 0.6500\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6658 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6296 - acc: 0.7150 - val_loss: 0.6658 - val_acc: 0.6400\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6656 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6259 - acc: 0.7050 - val_loss: 0.6656 - val_acc: 0.6400\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6654 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6235 - acc: 0.7250 - val_loss: 0.6654 - val_acc: 0.6500\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6651 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6237 - acc: 0.7300 - val_loss: 0.6651 - val_acc: 0.6500\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6649 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6238 - acc: 0.7275 - val_loss: 0.6649 - val_acc: 0.6400\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6648 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6269 - acc: 0.7325 - val_loss: 0.6648 - val_acc: 0.6400\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6646 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6224 - acc: 0.7250 - val_loss: 0.6646 - val_acc: 0.6400\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6644 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6225 - acc: 0.7350 - val_loss: 0.6644 - val_acc: 0.6500\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6642 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6193 - acc: 0.7400 - val_loss: 0.6642 - val_acc: 0.6500\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6641 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6188 - acc: 0.7275 - val_loss: 0.6641 - val_acc: 0.6400\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6638 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6195 - acc: 0.7175 - val_loss: 0.6638 - val_acc: 0.6500\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6636 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6180 - acc: 0.7150 - val_loss: 0.6636 - val_acc: 0.6400\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6635 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6210 - acc: 0.7200 - val_loss: 0.6635 - val_acc: 0.6400\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6633 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6211 - acc: 0.7325 - val_loss: 0.6633 - val_acc: 0.6500\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6632 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6148 - acc: 0.7325 - val_loss: 0.6632 - val_acc: 0.6400\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6629 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6158 - acc: 0.7200 - val_loss: 0.6629 - val_acc: 0.6500\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6626 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6150 - acc: 0.7425 - val_loss: 0.6626 - val_acc: 0.6600\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6624 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6130 - acc: 0.7350 - val_loss: 0.6624 - val_acc: 0.6600\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6621 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6147 - acc: 0.7375 - val_loss: 0.6621 - val_acc: 0.6500\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6618 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6112 - acc: 0.7350 - val_loss: 0.6618 - val_acc: 0.6500\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6616 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6172 - acc: 0.7150 - val_loss: 0.6616 - val_acc: 0.6500\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6615 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6113 - acc: 0.7325 - val_loss: 0.6615 - val_acc: 0.6500\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6615 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6102 - acc: 0.7275 - val_loss: 0.6615 - val_acc: 0.6400\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6612 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6098 - acc: 0.7425 - val_loss: 0.6612 - val_acc: 0.6400\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6606 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6103 - acc: 0.7575 - val_loss: 0.6606 - val_acc: 0.6500\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6604 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6095 - acc: 0.7450 - val_loss: 0.6604 - val_acc: 0.6500\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6601 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6101 - acc: 0.7400 - val_loss: 0.6601 - val_acc: 0.6500\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6599 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6122 - acc: 0.7300 - val_loss: 0.6599 - val_acc: 0.6500\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6597 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6084 - acc: 0.7525 - val_loss: 0.6597 - val_acc: 0.6500\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6595 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6098 - acc: 0.7425 - val_loss: 0.6595 - val_acc: 0.6600\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6593 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6059 - acc: 0.7450 - val_loss: 0.6593 - val_acc: 0.6600\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6590 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6050 - acc: 0.7500 - val_loss: 0.6590 - val_acc: 0.6600\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6590 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6059 - acc: 0.7375 - val_loss: 0.6590 - val_acc: 0.6500\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6591 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6060 - acc: 0.7350 - val_loss: 0.6591 - val_acc: 0.6300\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6583 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6093 - acc: 0.7275 - val_loss: 0.6583 - val_acc: 0.6500\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6581 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6034 - acc: 0.7500 - val_loss: 0.6581 - val_acc: 0.6400\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6580 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6031 - acc: 0.7325 - val_loss: 0.6580 - val_acc: 0.6300\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6577 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6022 - acc: 0.7500 - val_loss: 0.6577 - val_acc: 0.6300\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6575 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6007 - acc: 0.7425 - val_loss: 0.6575 - val_acc: 0.6400\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6573 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6014 - acc: 0.7500 - val_loss: 0.6573 - val_acc: 0.6400\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6568 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5983 - acc: 0.7625 - val_loss: 0.6568 - val_acc: 0.6400\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6565 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6014 - acc: 0.7400 - val_loss: 0.6565 - val_acc: 0.6400\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6565 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5981 - acc: 0.7475 - val_loss: 0.6565 - val_acc: 0.6400\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6560 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5960 - acc: 0.7600 - val_loss: 0.6560 - val_acc: 0.6500\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6559 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5994 - acc: 0.7525 - val_loss: 0.6559 - val_acc: 0.6500\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6557 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6013 - acc: 0.7525 - val_loss: 0.6557 - val_acc: 0.6500\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6558 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6004 - acc: 0.7450 - val_loss: 0.6558 - val_acc: 0.6500\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6551 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5942 - acc: 0.7475 - val_loss: 0.6551 - val_acc: 0.6500\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6548 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5963 - acc: 0.7575 - val_loss: 0.6548 - val_acc: 0.6500\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6547 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5933 - acc: 0.7475 - val_loss: 0.6547 - val_acc: 0.6500\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6544 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5948 - acc: 0.7550 - val_loss: 0.6544 - val_acc: 0.6500\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6542 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5955 - acc: 0.7550 - val_loss: 0.6542 - val_acc: 0.6500\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6538 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5932 - acc: 0.7575 - val_loss: 0.6538 - val_acc: 0.6600\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6536 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5919 - acc: 0.7575 - val_loss: 0.6536 - val_acc: 0.6600\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6535 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5914 - acc: 0.7600 - val_loss: 0.6535 - val_acc: 0.6500\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6531 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5925 - acc: 0.7600 - val_loss: 0.6531 - val_acc: 0.6300\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6533 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5907 - acc: 0.7525 - val_loss: 0.6533 - val_acc: 0.6400\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6526 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5893 - acc: 0.7575 - val_loss: 0.6526 - val_acc: 0.6700\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6525 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5904 - acc: 0.7600 - val_loss: 0.6525 - val_acc: 0.6600\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6528 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5903 - acc: 0.7400 - val_loss: 0.6528 - val_acc: 0.6500\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6522 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5881 - acc: 0.7650 - val_loss: 0.6522 - val_acc: 0.6400\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6521 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5896 - acc: 0.7625 - val_loss: 0.6521 - val_acc: 0.6400\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6520 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5893 - acc: 0.7600 - val_loss: 0.6520 - val_acc: 0.6400\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6518 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5914 - acc: 0.7500 - val_loss: 0.6518 - val_acc: 0.6400\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6512 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5849 - acc: 0.7600 - val_loss: 0.6512 - val_acc: 0.6600\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6515 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5865 - acc: 0.7500 - val_loss: 0.6515 - val_acc: 0.6400\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6508 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5842 - acc: 0.7675 - val_loss: 0.6508 - val_acc: 0.6600\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6505 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5857 - acc: 0.7625 - val_loss: 0.6505 - val_acc: 0.6500\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6504 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5857 - acc: 0.7575 - val_loss: 0.6504 - val_acc: 0.6700\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6505 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5854 - acc: 0.7600 - val_loss: 0.6505 - val_acc: 0.6400\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6502 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5864 - acc: 0.7475 - val_loss: 0.6502 - val_acc: 0.6400\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6502 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5834 - acc: 0.7500 - val_loss: 0.6502 - val_acc: 0.6400\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6500 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5825 - acc: 0.7575 - val_loss: 0.6500 - val_acc: 0.6400\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6499 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5818 - acc: 0.7550 - val_loss: 0.6499 - val_acc: 0.6400\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6494 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5818 - acc: 0.7625 - val_loss: 0.6494 - val_acc: 0.6600\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6496 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5790 - acc: 0.7650 - val_loss: 0.6496 - val_acc: 0.6400\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6490 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5792 - acc: 0.7700 - val_loss: 0.6490 - val_acc: 0.6700\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6492 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5804 - acc: 0.7575 - val_loss: 0.6492 - val_acc: 0.6400\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6486 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5804 - acc: 0.7725 - val_loss: 0.6486 - val_acc: 0.6700\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6488 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5789 - acc: 0.7475 - val_loss: 0.6488 - val_acc: 0.6400\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6483 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5814 - acc: 0.7600 - val_loss: 0.6483 - val_acc: 0.6700\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6482 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5787 - acc: 0.7575 - val_loss: 0.6482 - val_acc: 0.6700\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6480 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5761 - acc: 0.7700 - val_loss: 0.6480 - val_acc: 0.6700\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6479 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5788 - acc: 0.7625 - val_loss: 0.6479 - val_acc: 0.6600\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6478 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5788 - acc: 0.7550 - val_loss: 0.6478 - val_acc: 0.6500\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6480 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5778 - acc: 0.7475 - val_loss: 0.6480 - val_acc: 0.6400\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6476 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5778 - acc: 0.7525 - val_loss: 0.6476 - val_acc: 0.6400\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6473 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5746 - acc: 0.7675 - val_loss: 0.6473 - val_acc: 0.6400\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6469 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5769 - acc: 0.7625 - val_loss: 0.6469 - val_acc: 0.6700\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6467 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5752 - acc: 0.7600 - val_loss: 0.6467 - val_acc: 0.6700\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6467 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5758 - acc: 0.7500 - val_loss: 0.6467 - val_acc: 0.6500\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6465 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5749 - acc: 0.7650 - val_loss: 0.6465 - val_acc: 0.6700\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6468 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5701 - acc: 0.7625 - val_loss: 0.6468 - val_acc: 0.6400\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6461 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5721 - acc: 0.7675 - val_loss: 0.6461 - val_acc: 0.6700\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6459 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5729 - acc: 0.7700 - val_loss: 0.6459 - val_acc: 0.6700\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6458 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5701 - acc: 0.7625 - val_loss: 0.6458 - val_acc: 0.6700\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6458 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5707 - acc: 0.7625 - val_loss: 0.6458 - val_acc: 0.6500\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6455 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5762 - acc: 0.7475 - val_loss: 0.6455 - val_acc: 0.6700\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6453 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5691 - acc: 0.7700 - val_loss: 0.6453 - val_acc: 0.6700\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6452 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5708 - acc: 0.7700 - val_loss: 0.6452 - val_acc: 0.6700\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6451 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5691 - acc: 0.7750 - val_loss: 0.6451 - val_acc: 0.6600\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6449 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5689 - acc: 0.7725 - val_loss: 0.6449 - val_acc: 0.6600\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6449 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5690 - acc: 0.7725 - val_loss: 0.6449 - val_acc: 0.6500\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6445 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5685 - acc: 0.7625 - val_loss: 0.6445 - val_acc: 0.6700\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6446 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5739 - acc: 0.7600 - val_loss: 0.6446 - val_acc: 0.6500\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6445 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5717 - acc: 0.7650 - val_loss: 0.6445 - val_acc: 0.6500\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6444 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5723 - acc: 0.7700 - val_loss: 0.6444 - val_acc: 0.6500\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6441 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5678 - acc: 0.7650 - val_loss: 0.6441 - val_acc: 0.6600\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6440 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5681 - acc: 0.7700 - val_loss: 0.6440 - val_acc: 0.6700\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6438 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5645 - acc: 0.7800 - val_loss: 0.6438 - val_acc: 0.6700\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6439 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5650 - acc: 0.7775 - val_loss: 0.6439 - val_acc: 0.6500\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6437 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5657 - acc: 0.7725 - val_loss: 0.6437 - val_acc: 0.6500\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6435 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5664 - acc: 0.7650 - val_loss: 0.6435 - val_acc: 0.6700\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6434 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5675 - acc: 0.7600 - val_loss: 0.6434 - val_acc: 0.6500\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6435 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5624 - acc: 0.7700 - val_loss: 0.6435 - val_acc: 0.6500\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6434 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5642 - acc: 0.7725 - val_loss: 0.6434 - val_acc: 0.6500\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6433 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5659 - acc: 0.7850 - val_loss: 0.6433 - val_acc: 0.6500\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6430 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5616 - acc: 0.7850 - val_loss: 0.6430 - val_acc: 0.6500\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6428 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5660 - acc: 0.7650 - val_loss: 0.6428 - val_acc: 0.6700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgtyZLsvG_ls",
        "colab_type": "text"
      },
      "source": [
        "**Learning curves**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-ketbynHDhE",
        "colab_type": "code",
        "outputId": "852a7aef-9247-43f5-b7b7-ea0845923857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "#get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy - 2')\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.title('Training and Validation Loss - 2')\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POHLQd9PPMLX",
        "colab_type": "code",
        "outputId": "0a502d26-007d-4778-bc58-cf41175595df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "_, acc = model.evaluate_generator(generator=train_generator)\n",
        "print('Accuracy on training set: {}%'.format(round(acc, 3)*100))\n",
        "\n",
        "_, acc = model.evaluate_generator(generator=validation_generator)\n",
        "print('Accuracy on validation set: {}%'.format(round(acc, 3)*100))\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 77.49999761581421%\n",
            "Accuracy on validation set: 67.00000166893005%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeN1ZqTAM42i",
        "colab_type": "text"
      },
      "source": [
        "**Export Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkUfyxqDM3NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keras library import  for Saving and loading model and weights\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwvXWQnGEENs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = model.to_json()\n",
        "\n",
        "\n",
        "with open(\"drive/Colab Notebooks/smodel/s2_model_num.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"drive/Colab Notebooks/smodel/s2_model_num.h5\")\n",
        "model.save('drive/Colab Notebooks/smodel/s2_model_num.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnyvG-KY1Xt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}