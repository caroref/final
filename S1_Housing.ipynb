{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S1 Housing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caroref/final/blob/master/S1_Housing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ascUYWYXFlTS",
        "colab_type": "code",
        "outputId": "66f5ca4c-787c-4ff5-a462-af99661aab82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TensorFlow version is \", tf.__version__)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0603 22:25:03.242702 140482152421248 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version is  1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs-4SeS3Fpc5",
        "colab_type": "code",
        "outputId": "9e44b718-212b-407d-9ea8-9dda32cfd269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Allow Colab to access drive\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "#linking Colab to use drive as if local\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDNLdWVxP2jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = 'drive/Colab/EV/Base/'\n",
        "like = os.path.join(base_dir, 'Like')\n",
        "dislike = os.path.join(base_dir, 'Dislike')\n",
        "\n",
        "# subdirectories for train/test/valid splits\n",
        "trn_dir = 'drive/Colab/EV/S1/Train/'\n",
        "tst_dir = 'drive/Colab/EV/S1/Test/'\n",
        "\n",
        "trn_like = os.path.join(trn_dir, 'Like')\n",
        "tst_like = os.path.join(tst_dir, 'Like')\n",
        "\n",
        "trn_disl = os.path.join(trn_dir, 'Dislike')\n",
        "tst_disl = os.path.join(tst_dir, 'Dislike')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIj0GyA3KOsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# numpy split\n",
        "#trn1, val1, tst1 = np.split(like_f, [int(.6*len(like_f)), int(.8*len(like_f))])\n",
        "#trn2, val2, tst2 = np.split(disl_f, [int(.6*len(disl_f)), int(.8*len(disl_f))])\n",
        "\n",
        "#print ('training like images:', len(trn1))\n",
        "#print ('validation like images:', len(val1))\n",
        "#print ('test like images:', len(tst1))\n",
        "#print ()\n",
        "#print ('training dislike images:', len(trn2))\n",
        "#print ('validation dislike images:', len(val2))\n",
        "#print ('test dislike images:', len(tst2))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-bXAWyl7fuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#copy files to folders\n",
        "\n",
        "#for file in trn1:\n",
        "  #filename = os.path.join(like,file)\n",
        "  #print(file)\n",
        "  #shutil.copy(filename, trn_like)\n",
        "  #print(trn_like)\n",
        "\n",
        "#for file in val1:\n",
        "  #filename = os.path.join(like,file)\n",
        "  #shutil.copy(filename, val_like)\n",
        "\n",
        "#for file in tst1:\n",
        "  #filename = os.path.join(like,file)\n",
        "  #shutil.copy(filename, tst_like)\n",
        "\n",
        "#for file in trn2:\n",
        "  #filename = os.path.join(dislike,file)\n",
        "  #shutil.copy(filename, trn_disl)\n",
        "\n",
        "#for file in val2:\n",
        "  #filename = os.path.join(dislike,file)\n",
        "  #shutil.copy(filename, val_disl)\n",
        "\n",
        "#for file in tst2:\n",
        "  #filename = os.path.join(dislike,file)\n",
        "  #shutil.copy(filename, tst_disl)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo3GVjj1FsEn",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5BtzOSmbZB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All images will be resized to image_size\n",
        "\n",
        "image_size = 640 \n",
        "batch_size = 100\n",
        "\n",
        "# create Image Data Generator for Image Data Augmentation\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "                rescale=1./255, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTvB7EieGDAN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a481708d-4c73-41d9-f2bd-3553c1de1e22"
      },
      "source": [
        "# Create the train generator and specify where the train dataset directory, \n",
        "#  image size, batch size and it's a binary classification.\n",
        "# Create the validation generator with similar approach as \n",
        "#  the train generator with the flow_from_directory() method.\n",
        "\n",
        "\n",
        "# Flow training images in batches using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,  \n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode ='binary',\n",
        "                subset = 'training') \n",
        "\n",
        "# Flow validation images in batches using validation_datagen generator\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,\n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode='binary',\n",
        "                subset = 'validation')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ur5r8uGWQO",
        "colab_type": "text"
      },
      "source": [
        "# Create the base model from the pre-trained convnets\n",
        "\n",
        "Using the output of last layer (bottleneck layer). Keep other layers frozen (these are specific to their training, less useful here?) Bottleneck layer should be more general.\n",
        "\n",
        "Using Inception v3 model with weights trained on ImageNet.\n",
        "The argument, include_top=False, means that we have a network that isn't including classification layers at the top. This is good for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCLmagoGbc3",
        "colab_type": "code",
        "outputId": "31071cc3-c554-4988-9b5f-1d2a87201fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "IMG_SHAPE = (image_size, image_size, 3)\n",
        "\n",
        "base_model = keras.applications.inception_v3.InceptionV3(\n",
        "             include_top=False, weights='imagenet', input_shape=IMG_SHAPE)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0603 22:28:05.442716 140482152421248 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wXzV2zlGduw",
        "colab_type": "text"
      },
      "source": [
        "# Feature extraction\n",
        "We will freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "It's important to freeze the convolutional based before we compile and train the model. By freezing (or setting layer.trainable = False), we prevent the weights in these layers from being updated during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCk3KhmGGhaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11821
        },
        "outputId": "45b506db-7224-442c-b920-fd8d4b32e3b5"
      },
      "source": [
        "#freeze base\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# Look at the base model architecture\n",
        "base_model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 640, 640, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 319, 319, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 319, 319, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 319, 319, 32) 0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 317, 317, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 317, 317, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 317, 317, 32) 0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 317, 317, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 317, 317, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 317, 317, 64) 0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 158, 158, 64) 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 158, 158, 80) 5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 158, 158, 80) 240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 158, 158, 80) 0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 156, 156, 192 138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 156, 156, 192 576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 156, 156, 192 0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 77, 77, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_8 (Batch (None, 77, 77, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 77, 77, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 77, 77, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_6 (Batch (None, 77, 77, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_9 (Batch (None, 77, 77, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 77, 77, 48)   0           batch_normalization_v1_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 77, 77, 96)   0           batch_normalization_v1_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 77, 77, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 77, 77, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 77, 77, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 77, 77, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_5 (Batch (None, 77, 77, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_7 (Batch (None, 77, 77, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_10 (Batc (None, 77, 77, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_11 (Batc (None, 77, 77, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 77, 77, 32)   0           batch_normalization_v1_11[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 77, 77, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_15 (Batc (None, 77, 77, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_15[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 77, 77, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 77, 77, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_13 (Batc (None, 77, 77, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_16 (Batc (None, 77, 77, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_13[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_16[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 77, 77, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 77, 77, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 77, 77, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 77, 77, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_12 (Batc (None, 77, 77, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_14 (Batc (None, 77, 77, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_17 (Batc (None, 77, 77, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_18 (Batc (None, 77, 77, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_12[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_14[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_17[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_18[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 77, 77, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_22 (Batc (None, 77, 77, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_22[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 77, 77, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 77, 77, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_20 (Batc (None, 77, 77, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_23 (Batc (None, 77, 77, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_20[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_23[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 77, 77, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 77, 77, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 77, 77, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 77, 77, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_19 (Batc (None, 77, 77, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_21 (Batc (None, 77, 77, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_24 (Batc (None, 77, 77, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_25 (Batc (None, 77, 77, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_19[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_21[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_24[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_25[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 77, 77, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 77, 77, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_27 (Batc (None, 77, 77, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_27[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 77, 77, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_28 (Batc (None, 77, 77, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_28[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 38, 38, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 38, 38, 96)   82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_26 (Batc (None, 38, 38, 384)  1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_29 (Batc (None, 38, 38, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 38, 38, 384)  0           batch_normalization_v1_26[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 38, 38, 96)   0           batch_normalization_v1_29[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 38, 38, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 38, 38, 768)  0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_34 (Batc (None, 38, 38, 128)  384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_34[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 38, 38, 128)  114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_35 (Batc (None, 38, 38, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_35[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 38, 38, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_31 (Batc (None, 38, 38, 128)  384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_36 (Batc (None, 38, 38, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_31[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_36[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 38, 38, 128)  114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 38, 38, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_32 (Batc (None, 38, 38, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_37 (Batc (None, 38, 38, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_32[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_37[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 38, 38, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 38, 38, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 38, 38, 192)  172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 38, 38, 192)  172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_30 (Batc (None, 38, 38, 192)  576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_33 (Batc (None, 38, 38, 192)  576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_38 (Batc (None, 38, 38, 192)  576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_39 (Batc (None, 38, 38, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_30[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_33[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_38[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_39[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 38, 38, 768)  0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_44 (Batc (None, 38, 38, 160)  480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_44[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 38, 38, 160)  179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_45 (Batc (None, 38, 38, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_45[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 38, 38, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_41 (Batc (None, 38, 38, 160)  480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_46 (Batc (None, 38, 38, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_41[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_46[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 38, 38, 160)  179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 38, 38, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_42 (Batc (None, 38, 38, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_47 (Batc (None, 38, 38, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_42[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_47[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 38, 38, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 38, 38, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 38, 38, 192)  215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 38, 38, 192)  215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_40 (Batc (None, 38, 38, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_43 (Batc (None, 38, 38, 192)  576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_48 (Batc (None, 38, 38, 192)  576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_49 (Batc (None, 38, 38, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_40[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_43[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_48[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_49[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 38, 38, 768)  0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_54 (Batc (None, 38, 38, 160)  480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_54[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 38, 38, 160)  179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_55 (Batc (None, 38, 38, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_55[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 38, 38, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_51 (Batc (None, 38, 38, 160)  480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_56 (Batc (None, 38, 38, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_51[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_56[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 38, 38, 160)  179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 38, 38, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_52 (Batc (None, 38, 38, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_57 (Batc (None, 38, 38, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_52[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_57[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 38, 38, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 38, 38, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 38, 38, 192)  215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 38, 38, 192)  215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_50 (Batc (None, 38, 38, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_53 (Batc (None, 38, 38, 192)  576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_58 (Batc (None, 38, 38, 192)  576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_59 (Batc (None, 38, 38, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_50[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_53[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_58[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_59[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 38, 38, 768)  0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_64 (Batc (None, 38, 38, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_64[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 38, 38, 192)  258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_65 (Batc (None, 38, 38, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_65[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 38, 38, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_61 (Batc (None, 38, 38, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_66 (Batc (None, 38, 38, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_61[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_66[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 38, 38, 192)  258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 38, 38, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_62 (Batc (None, 38, 38, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_67 (Batc (None, 38, 38, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_62[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_67[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 38, 38, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 38, 38, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 38, 38, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_60 (Batc (None, 38, 38, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_63 (Batc (None, 38, 38, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_68 (Batc (None, 38, 38, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_69 (Batc (None, 38, 38, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_60[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_63[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_68[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_69[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 38, 38, 768)  0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_72 (Batc (None, 38, 38, 192)  576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_72[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 38, 38, 192)  258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_73 (Batc (None, 38, 38, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_73[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 38, 38, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_70 (Batc (None, 38, 38, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_74 (Batc (None, 38, 38, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_70[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_74[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 18, 18, 320)  552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 18, 18, 192)  331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_71 (Batc (None, 18, 18, 320)  960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_75 (Batc (None, 18, 18, 192)  576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_71[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_75[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 18, 18, 768)  0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 18, 18, 1280) 0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 18, 18, 448)  573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_80 (Batc (None, 18, 18, 448)  1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_80[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 18, 18, 384)  491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 18, 18, 384)  1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_77 (Batc (None, 18, 18, 384)  1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_81 (Batc (None, 18, 18, 384)  1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_77[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_81[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 18, 18, 1280) 0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 18, 18, 320)  409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_78 (Batc (None, 18, 18, 384)  1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_79 (Batc (None, 18, 18, 384)  1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_82 (Batc (None, 18, 18, 384)  1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_83 (Batc (None, 18, 18, 384)  1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 18, 18, 192)  245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_76 (Batc (None, 18, 18, 320)  960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_78[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_79[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_82[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_83[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_84 (Batc (None, 18, 18, 192)  576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_76[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 18, 18, 768)  0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 18, 18, 768)  0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_84[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 18, 18, 2048) 0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 18, 18, 448)  917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_89 (Batc (None, 18, 18, 448)  1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_89[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 18, 18, 384)  786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 18, 18, 384)  1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_86 (Batc (None, 18, 18, 384)  1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_90 (Batc (None, 18, 18, 384)  1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_86[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_90[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 18, 18, 2048) 0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 18, 18, 320)  655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_87 (Batc (None, 18, 18, 384)  1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_88 (Batc (None, 18, 18, 384)  1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_91 (Batc (None, 18, 18, 384)  1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_92 (Batc (None, 18, 18, 384)  1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 18, 18, 192)  393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_85 (Batc (None, 18, 18, 320)  960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_87[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_88[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_91[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_92[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_93 (Batc (None, 18, 18, 192)  576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_85[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 18, 18, 768)  0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 18, 18, 768)  0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_93[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 18, 18, 2048) 0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 0\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUc1ad5qGnMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add a classification head\n",
        "#adding a few layers on top of the base model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  keras.layers.GlobalAveragePooling2D(),\n",
        "  keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "#relatively basic layer. no deep neural networks (all happening beforehand). Similar to a logit regression. plain classification\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbhLA3TGr1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "6761568f-f7e7-4f12-c4f5-f99e73d19591"
      },
      "source": [
        "#Compile the model (Must do before training!)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 18, 18, 2048)      21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 21,804,833\n",
            "Trainable params: 2,049\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th655DVEGwop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "812caeae-d89e-4ba3-c59e-6db4041e2fec"
      },
      "source": [
        "#These 2K trainable parameters are divided among 2 TensorFlow Variable objects, the weights and biases of the two dense layers:\n",
        "\n",
        "\n",
        "len(model.trainable_variables)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZeMfEwoG3wm",
        "colab_type": "text"
      },
      "source": [
        "**Train the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNgpgcJjG8xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10792
        },
        "outputId": "3c4541c5-68f5-4bd4-b53b-8bff210c44bc"
      },
      "source": [
        "epochs = 200\n",
        "steps_per_epoch = train_generator.n // batch_size\n",
        "validation_steps = validation_generator.n // batch_size\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch = steps_per_epoch,\n",
        "                              epochs=epochs,\n",
        "                              workers=4,\n",
        "                              validation_data=validation_generator,\n",
        "                              validation_steps=validation_steps)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0603 22:28:21.521745 140482152421248 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 52s 52s/step - loss: 0.6823 - acc: 0.5400\n",
            "4/4 [==============================] - 126s 32s/step - loss: 0.6956 - acc: 0.5050 - val_loss: 0.6823 - val_acc: 0.5400\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6822 - acc: 0.5300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6914 - acc: 0.5025 - val_loss: 0.6822 - val_acc: 0.5300\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 68s 68s/step - loss: 0.6792 - acc: 0.5500\n",
            "4/4 [==============================] - 75s 19s/step - loss: 0.6914 - acc: 0.5375 - val_loss: 0.6792 - val_acc: 0.5500\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 31s 31s/step - loss: 0.6798 - acc: 0.5500\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.6879 - acc: 0.5325 - val_loss: 0.6798 - val_acc: 0.5500\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 59s 59s/step - loss: 0.6778 - acc: 0.5800\n",
            "4/4 [==============================] - 96s 24s/step - loss: 0.6851 - acc: 0.5625 - val_loss: 0.6778 - val_acc: 0.5800\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 13s 13s/step - loss: 0.6780 - acc: 0.5600\n",
            "4/4 [==============================] - 21s 5s/step - loss: 0.6846 - acc: 0.5650 - val_loss: 0.6780 - val_acc: 0.5600\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 62s 62s/step - loss: 0.6773 - acc: 0.5800\n",
            "4/4 [==============================] - 106s 27s/step - loss: 0.6837 - acc: 0.5650 - val_loss: 0.6773 - val_acc: 0.5800\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6772 - acc: 0.5700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6826 - acc: 0.5575 - val_loss: 0.6772 - val_acc: 0.5700\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6772 - acc: 0.5700\n",
            "4/4 [==============================] - 36s 9s/step - loss: 0.6786 - acc: 0.5625 - val_loss: 0.6772 - val_acc: 0.5700\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6766 - acc: 0.5700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6814 - acc: 0.5725 - val_loss: 0.6766 - val_acc: 0.5700\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6759 - acc: 0.5700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6768 - acc: 0.6125 - val_loss: 0.6759 - val_acc: 0.5700\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6749 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6771 - acc: 0.5875 - val_loss: 0.6749 - val_acc: 0.6000\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6740 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6758 - acc: 0.5925 - val_loss: 0.6740 - val_acc: 0.6000\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6748 - acc: 0.5600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6739 - acc: 0.5850 - val_loss: 0.6748 - val_acc: 0.5600\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6740 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6741 - acc: 0.6150 - val_loss: 0.6740 - val_acc: 0.6000\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6729 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6707 - acc: 0.6175 - val_loss: 0.6729 - val_acc: 0.5900\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6720 - acc: 0.5900\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6687 - acc: 0.6550 - val_loss: 0.6720 - val_acc: 0.5900\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6700 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6688 - acc: 0.6375 - val_loss: 0.6700 - val_acc: 0.6400\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6699 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6679 - acc: 0.6300 - val_loss: 0.6699 - val_acc: 0.6100\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6690 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6645 - acc: 0.6500 - val_loss: 0.6690 - val_acc: 0.6400\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6678 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6648 - acc: 0.6550 - val_loss: 0.6678 - val_acc: 0.6500\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6685 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6639 - acc: 0.6325 - val_loss: 0.6685 - val_acc: 0.6100\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6664 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6635 - acc: 0.6525 - val_loss: 0.6664 - val_acc: 0.6600\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6660 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6604 - acc: 0.6400 - val_loss: 0.6660 - val_acc: 0.6400\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6653 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6601 - acc: 0.6525 - val_loss: 0.6653 - val_acc: 0.6700\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6649 - acc: 0.6600\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6584 - acc: 0.6500 - val_loss: 0.6649 - val_acc: 0.6600\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6645 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6569 - acc: 0.6675 - val_loss: 0.6645 - val_acc: 0.6600\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6652 - acc: 0.6500\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6601 - acc: 0.6350 - val_loss: 0.6652 - val_acc: 0.6500\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6643 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6540 - acc: 0.6725 - val_loss: 0.6643 - val_acc: 0.6200\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6643 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6534 - acc: 0.6625 - val_loss: 0.6643 - val_acc: 0.6300\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6649 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6537 - acc: 0.6650 - val_loss: 0.6649 - val_acc: 0.6400\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6644 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6524 - acc: 0.6900 - val_loss: 0.6644 - val_acc: 0.6400\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6632 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6511 - acc: 0.6875 - val_loss: 0.6632 - val_acc: 0.6300\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6623 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6498 - acc: 0.6850 - val_loss: 0.6623 - val_acc: 0.6200\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6620 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6497 - acc: 0.6775 - val_loss: 0.6620 - val_acc: 0.6200\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6617 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6476 - acc: 0.6950 - val_loss: 0.6617 - val_acc: 0.6300\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6612 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6463 - acc: 0.6850 - val_loss: 0.6612 - val_acc: 0.6300\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6605 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6464 - acc: 0.6975 - val_loss: 0.6605 - val_acc: 0.6300\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6607 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6449 - acc: 0.6900 - val_loss: 0.6607 - val_acc: 0.6100\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6608 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6429 - acc: 0.6950 - val_loss: 0.6608 - val_acc: 0.6200\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6593 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6428 - acc: 0.7100 - val_loss: 0.6593 - val_acc: 0.6500\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6586 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6409 - acc: 0.7000 - val_loss: 0.6586 - val_acc: 0.6400\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6576 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6447 - acc: 0.6850 - val_loss: 0.6576 - val_acc: 0.6500\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6569 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6426 - acc: 0.7050 - val_loss: 0.6569 - val_acc: 0.6500\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6563 - acc: 0.6500\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6389 - acc: 0.7000 - val_loss: 0.6563 - val_acc: 0.6500\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6556 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6363 - acc: 0.7225 - val_loss: 0.6556 - val_acc: 0.6600\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6548 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6363 - acc: 0.7250 - val_loss: 0.6548 - val_acc: 0.6500\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6538 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6347 - acc: 0.7300 - val_loss: 0.6538 - val_acc: 0.6400\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6532 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6364 - acc: 0.7050 - val_loss: 0.6532 - val_acc: 0.6400\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6525 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6344 - acc: 0.7225 - val_loss: 0.6525 - val_acc: 0.6400\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6519 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6342 - acc: 0.7100 - val_loss: 0.6519 - val_acc: 0.6400\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6511 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6317 - acc: 0.7300 - val_loss: 0.6511 - val_acc: 0.6400\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6508 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6328 - acc: 0.7025 - val_loss: 0.6508 - val_acc: 0.6500\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6499 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6307 - acc: 0.7275 - val_loss: 0.6499 - val_acc: 0.6500\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6492 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6288 - acc: 0.7325 - val_loss: 0.6492 - val_acc: 0.6600\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6490 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6289 - acc: 0.7075 - val_loss: 0.6490 - val_acc: 0.6700\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6483 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6271 - acc: 0.7200 - val_loss: 0.6483 - val_acc: 0.6600\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6478 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6285 - acc: 0.7200 - val_loss: 0.6478 - val_acc: 0.6600\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6472 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6267 - acc: 0.7275 - val_loss: 0.6472 - val_acc: 0.6600\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6464 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6257 - acc: 0.7300 - val_loss: 0.6464 - val_acc: 0.6500\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6460 - acc: 0.6600\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6275 - acc: 0.7100 - val_loss: 0.6460 - val_acc: 0.6600\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6456 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6235 - acc: 0.7325 - val_loss: 0.6456 - val_acc: 0.6500\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6451 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6245 - acc: 0.7325 - val_loss: 0.6451 - val_acc: 0.6500\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6449 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6275 - acc: 0.7225 - val_loss: 0.6449 - val_acc: 0.6500\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6448 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6218 - acc: 0.7200 - val_loss: 0.6448 - val_acc: 0.6500\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6442 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6240 - acc: 0.7375 - val_loss: 0.6442 - val_acc: 0.6600\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6442 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6209 - acc: 0.7150 - val_loss: 0.6442 - val_acc: 0.6500\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6439 - acc: 0.6500\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6215 - acc: 0.7275 - val_loss: 0.6439 - val_acc: 0.6500\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6439 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6195 - acc: 0.7200 - val_loss: 0.6439 - val_acc: 0.6800\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6435 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6205 - acc: 0.7225 - val_loss: 0.6435 - val_acc: 0.6800\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6433 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6174 - acc: 0.7400 - val_loss: 0.6433 - val_acc: 0.6800\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6425 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6157 - acc: 0.7500 - val_loss: 0.6425 - val_acc: 0.6600\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6425 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6141 - acc: 0.7375 - val_loss: 0.6425 - val_acc: 0.6800\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6419 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6136 - acc: 0.7475 - val_loss: 0.6419 - val_acc: 0.6700\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6414 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6168 - acc: 0.7475 - val_loss: 0.6414 - val_acc: 0.6600\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6411 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6151 - acc: 0.7300 - val_loss: 0.6411 - val_acc: 0.6600\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6412 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6144 - acc: 0.7175 - val_loss: 0.6412 - val_acc: 0.6800\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6408 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6126 - acc: 0.7450 - val_loss: 0.6408 - val_acc: 0.6800\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6401 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6147 - acc: 0.7425 - val_loss: 0.6401 - val_acc: 0.6700\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6398 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6101 - acc: 0.7450 - val_loss: 0.6398 - val_acc: 0.6700\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6395 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6121 - acc: 0.7275 - val_loss: 0.6395 - val_acc: 0.6700\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6393 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6130 - acc: 0.7200 - val_loss: 0.6393 - val_acc: 0.6700\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6391 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6080 - acc: 0.7300 - val_loss: 0.6391 - val_acc: 0.6700\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6390 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6092 - acc: 0.7325 - val_loss: 0.6390 - val_acc: 0.6700\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6388 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6074 - acc: 0.7450 - val_loss: 0.6388 - val_acc: 0.6700\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6388 - acc: 0.6900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6075 - acc: 0.7375 - val_loss: 0.6388 - val_acc: 0.6900\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6378 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6075 - acc: 0.7450 - val_loss: 0.6378 - val_acc: 0.6700\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6375 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6054 - acc: 0.7425 - val_loss: 0.6375 - val_acc: 0.6700\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6371 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6062 - acc: 0.7200 - val_loss: 0.6371 - val_acc: 0.6700\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6369 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6070 - acc: 0.7325 - val_loss: 0.6369 - val_acc: 0.6700\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6367 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6028 - acc: 0.7450 - val_loss: 0.6367 - val_acc: 0.6700\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6363 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6015 - acc: 0.7500 - val_loss: 0.6363 - val_acc: 0.6700\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6360 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6015 - acc: 0.7450 - val_loss: 0.6360 - val_acc: 0.6700\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6359 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6005 - acc: 0.7250 - val_loss: 0.6359 - val_acc: 0.6700\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6355 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6018 - acc: 0.7525 - val_loss: 0.6355 - val_acc: 0.6700\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6353 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6024 - acc: 0.7300 - val_loss: 0.6353 - val_acc: 0.6700\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6352 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5985 - acc: 0.7475 - val_loss: 0.6352 - val_acc: 0.6700\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6351 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6003 - acc: 0.7300 - val_loss: 0.6351 - val_acc: 0.6800\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6349 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5999 - acc: 0.7475 - val_loss: 0.6349 - val_acc: 0.6800\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6345 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5975 - acc: 0.7475 - val_loss: 0.6345 - val_acc: 0.6700\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6343 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5972 - acc: 0.7500 - val_loss: 0.6343 - val_acc: 0.6800\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6340 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5937 - acc: 0.7675 - val_loss: 0.6340 - val_acc: 0.6700\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6341 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5973 - acc: 0.7300 - val_loss: 0.6341 - val_acc: 0.6800\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6337 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5945 - acc: 0.7475 - val_loss: 0.6337 - val_acc: 0.6800\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6332 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5958 - acc: 0.7475 - val_loss: 0.6332 - val_acc: 0.6800\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6330 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5953 - acc: 0.7350 - val_loss: 0.6330 - val_acc: 0.6700\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6327 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5949 - acc: 0.7400 - val_loss: 0.6327 - val_acc: 0.6700\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6330 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5949 - acc: 0.7400 - val_loss: 0.6330 - val_acc: 0.6800\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6326 - acc: 0.6800\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5913 - acc: 0.7450 - val_loss: 0.6326 - val_acc: 0.6800\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6327 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5914 - acc: 0.7475 - val_loss: 0.6327 - val_acc: 0.6800\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6329 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5936 - acc: 0.7300 - val_loss: 0.6329 - val_acc: 0.6700\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6321 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5882 - acc: 0.7625 - val_loss: 0.6321 - val_acc: 0.6800\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6314 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5908 - acc: 0.7600 - val_loss: 0.6314 - val_acc: 0.6700\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6314 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5936 - acc: 0.7400 - val_loss: 0.6314 - val_acc: 0.6700\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6310 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5883 - acc: 0.7475 - val_loss: 0.6310 - val_acc: 0.6700\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6308 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5892 - acc: 0.7450 - val_loss: 0.6308 - val_acc: 0.6700\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6308 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5866 - acc: 0.7500 - val_loss: 0.6308 - val_acc: 0.6700\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6302 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5868 - acc: 0.7550 - val_loss: 0.6302 - val_acc: 0.6700\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6301 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5838 - acc: 0.7650 - val_loss: 0.6301 - val_acc: 0.6600\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6297 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5852 - acc: 0.7525 - val_loss: 0.6297 - val_acc: 0.6700\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6294 - acc: 0.6900\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5842 - acc: 0.7525 - val_loss: 0.6294 - val_acc: 0.6900\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6291 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5826 - acc: 0.7625 - val_loss: 0.6291 - val_acc: 0.6700\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6287 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5822 - acc: 0.7525 - val_loss: 0.6287 - val_acc: 0.6700\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6286 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5876 - acc: 0.7425 - val_loss: 0.6286 - val_acc: 0.6700\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6285 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5826 - acc: 0.7500 - val_loss: 0.6285 - val_acc: 0.6700\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6282 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5799 - acc: 0.7675 - val_loss: 0.6282 - val_acc: 0.6700\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6282 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5807 - acc: 0.7475 - val_loss: 0.6282 - val_acc: 0.6700\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6279 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5812 - acc: 0.7700 - val_loss: 0.6279 - val_acc: 0.6700\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6278 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5805 - acc: 0.7525 - val_loss: 0.6278 - val_acc: 0.6600\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6273 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5791 - acc: 0.7625 - val_loss: 0.6273 - val_acc: 0.6700\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6270 - acc: 0.6900\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5777 - acc: 0.7550 - val_loss: 0.6270 - val_acc: 0.6900\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6268 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5787 - acc: 0.7450 - val_loss: 0.6268 - val_acc: 0.6700\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6268 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5782 - acc: 0.7575 - val_loss: 0.6268 - val_acc: 0.6700\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6268 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5786 - acc: 0.7525 - val_loss: 0.6268 - val_acc: 0.6700\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6265 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5756 - acc: 0.7550 - val_loss: 0.6265 - val_acc: 0.6700\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6261 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5759 - acc: 0.7575 - val_loss: 0.6261 - val_acc: 0.6700\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6266 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5757 - acc: 0.7375 - val_loss: 0.6266 - val_acc: 0.6600\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6260 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5782 - acc: 0.7550 - val_loss: 0.6260 - val_acc: 0.6700\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6256 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5787 - acc: 0.7550 - val_loss: 0.6256 - val_acc: 0.6700\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6255 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5736 - acc: 0.7775 - val_loss: 0.6255 - val_acc: 0.6700\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6252 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5727 - acc: 0.7575 - val_loss: 0.6252 - val_acc: 0.6800\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6259 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5751 - acc: 0.7575 - val_loss: 0.6259 - val_acc: 0.6600\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6252 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5748 - acc: 0.7700 - val_loss: 0.6252 - val_acc: 0.6700\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6248 - acc: 0.6800\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5732 - acc: 0.7525 - val_loss: 0.6248 - val_acc: 0.6800\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6247 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5717 - acc: 0.7550 - val_loss: 0.6247 - val_acc: 0.6700\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6250 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5697 - acc: 0.7675 - val_loss: 0.6250 - val_acc: 0.6800\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6250 - acc: 0.6600\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5705 - acc: 0.7750 - val_loss: 0.6250 - val_acc: 0.6600\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6246 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5707 - acc: 0.7675 - val_loss: 0.6246 - val_acc: 0.6800\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6244 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5681 - acc: 0.7650 - val_loss: 0.6244 - val_acc: 0.6800\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6240 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5682 - acc: 0.7650 - val_loss: 0.6240 - val_acc: 0.6700\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6237 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5703 - acc: 0.7600 - val_loss: 0.6237 - val_acc: 0.6700\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6239 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5735 - acc: 0.7500 - val_loss: 0.6239 - val_acc: 0.6800\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6233 - acc: 0.6800\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5690 - acc: 0.7600 - val_loss: 0.6233 - val_acc: 0.6800\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6232 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5664 - acc: 0.7700 - val_loss: 0.6232 - val_acc: 0.6700\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6232 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5679 - acc: 0.7500 - val_loss: 0.6232 - val_acc: 0.6700\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6229 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5652 - acc: 0.7525 - val_loss: 0.6229 - val_acc: 0.6700\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6227 - acc: 0.6800\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5687 - acc: 0.7550 - val_loss: 0.6227 - val_acc: 0.6800\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6227 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5641 - acc: 0.7650 - val_loss: 0.6227 - val_acc: 0.6700\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6227 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5671 - acc: 0.7575 - val_loss: 0.6227 - val_acc: 0.6800\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6228 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5641 - acc: 0.7600 - val_loss: 0.6228 - val_acc: 0.6700\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6226 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5617 - acc: 0.7675 - val_loss: 0.6226 - val_acc: 0.6700\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6221 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5635 - acc: 0.7650 - val_loss: 0.6221 - val_acc: 0.6700\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6219 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5618 - acc: 0.7725 - val_loss: 0.6219 - val_acc: 0.6700\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6217 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5633 - acc: 0.7725 - val_loss: 0.6217 - val_acc: 0.6700\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6217 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5646 - acc: 0.7650 - val_loss: 0.6217 - val_acc: 0.6700\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6219 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5663 - acc: 0.7675 - val_loss: 0.6219 - val_acc: 0.6700\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6214 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5619 - acc: 0.7725 - val_loss: 0.6214 - val_acc: 0.6700\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6213 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5598 - acc: 0.7625 - val_loss: 0.6213 - val_acc: 0.6700\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6216 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5595 - acc: 0.7800 - val_loss: 0.6216 - val_acc: 0.6700\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6209 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5603 - acc: 0.7475 - val_loss: 0.6209 - val_acc: 0.6700\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6213 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5604 - acc: 0.7800 - val_loss: 0.6213 - val_acc: 0.6700\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6211 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5608 - acc: 0.7700 - val_loss: 0.6211 - val_acc: 0.6700\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6209 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5655 - acc: 0.7375 - val_loss: 0.6209 - val_acc: 0.6700\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6204 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5596 - acc: 0.7625 - val_loss: 0.6204 - val_acc: 0.6700\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6203 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5557 - acc: 0.7700 - val_loss: 0.6203 - val_acc: 0.6700\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6206 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5583 - acc: 0.7650 - val_loss: 0.6206 - val_acc: 0.6700\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6201 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5583 - acc: 0.7750 - val_loss: 0.6201 - val_acc: 0.6600\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6205 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5584 - acc: 0.7500 - val_loss: 0.6205 - val_acc: 0.6700\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6200 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5569 - acc: 0.7725 - val_loss: 0.6200 - val_acc: 0.6700\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6199 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5550 - acc: 0.7625 - val_loss: 0.6199 - val_acc: 0.6700\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6196 - acc: 0.6600\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5551 - acc: 0.7625 - val_loss: 0.6196 - val_acc: 0.6600\n",
            "Epoch 182/200\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5548 - acc: 0.7700 - val_loss: 0.6194 - val_acc: 0.6700\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6192 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5565 - acc: 0.7600 - val_loss: 0.6192 - val_acc: 0.6800\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6191 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5537 - acc: 0.7725 - val_loss: 0.6191 - val_acc: 0.6700\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6193 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5553 - acc: 0.7650 - val_loss: 0.6193 - val_acc: 0.6700\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6189 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5544 - acc: 0.7750 - val_loss: 0.6189 - val_acc: 0.6600\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6189 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5497 - acc: 0.7750 - val_loss: 0.6189 - val_acc: 0.6600\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6189 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5499 - acc: 0.7650 - val_loss: 0.6189 - val_acc: 0.6700\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6189 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5551 - acc: 0.7700 - val_loss: 0.6189 - val_acc: 0.6700\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6190 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5532 - acc: 0.7675 - val_loss: 0.6190 - val_acc: 0.6700\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6183 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5488 - acc: 0.7750 - val_loss: 0.6183 - val_acc: 0.6700\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6181 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5512 - acc: 0.7700 - val_loss: 0.6181 - val_acc: 0.6700\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6184 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5501 - acc: 0.7675 - val_loss: 0.6184 - val_acc: 0.6700\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6186 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5514 - acc: 0.7675 - val_loss: 0.6186 - val_acc: 0.6600\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6179 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5466 - acc: 0.7750 - val_loss: 0.6179 - val_acc: 0.6600\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6179 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5537 - acc: 0.7675 - val_loss: 0.6179 - val_acc: 0.6700\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6177 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5476 - acc: 0.7750 - val_loss: 0.6177 - val_acc: 0.6600\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6177 - acc: 0.6700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5527 - acc: 0.7650 - val_loss: 0.6177 - val_acc: 0.6700\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6174 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5508 - acc: 0.7625 - val_loss: 0.6174 - val_acc: 0.6700\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6172 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5482 - acc: 0.7650 - val_loss: 0.6172 - val_acc: 0.6800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgtyZLsvG_ls",
        "colab_type": "text"
      },
      "source": [
        "**Learning curves**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-ketbynHDhE",
        "colab_type": "code",
        "outputId": "67c2d308-71c6-4fec-dda3-8d9985d8ad4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "#get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npY4RGtSVUkqpY8T/B3MLvFv/bJEnAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POHLQd9PPMLX",
        "colab_type": "code",
        "outputId": "9aea1cf8-1252-4bcf-a7d6-b5f2d723be03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "_, acc = model.evaluate_generator(generator=train_generator)\n",
        "print('Accuracy on training set: {}%'.format(round(acc, 3)*100))\n",
        "\n",
        "_, acc = model.evaluate_generator(generator=validation_generator)\n",
        "print('Accuracy on validation set: {}%'.format(round(acc, 3)*100))\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 77.7999997138977%\n",
            "Accuracy on validation set: 68.00000071525574%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeN1ZqTAM42i",
        "colab_type": "text"
      },
      "source": [
        "**Export Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkUfyxqDM3NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keras library import  for Saving and loading model and weights\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwvXWQnGEENs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = model.to_json()\n",
        "\n",
        "\n",
        "with open(\"drive/Colab_NB/smodel/s1_model_num.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"drive/Colab_NB/smodel/s1_model_num.h5\")\n",
        "model.save('drive/Colab_NB/smodel/s1_model_num.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}